{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab_siamese_networks.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Beno71/humpback-whale-classification/blob/master/Colab_siamese_network_Cross_Entropy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "caf9Pin0ddx6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os \n",
        "print(os.getcwd())\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KPOZ_GvIOpkR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# siamese networks for whale classification"
      ]
    },
    {
      "metadata": {
        "id": "miwpXH5NOpkX",
        "colab_type": "code",
        "outputId": "d0bd7a94-265b-478e-d585-dc1134526a5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "cell_type": "code",
      "source": [
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.rendered_html { font-size: 18px; }</style>\"))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>.rendered_html { font-size: 18px; }</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "swbuAkt8Opkg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import os, time, itertools\n",
        "from skimage import io, transform\n",
        "import skimage\n",
        "import glob\n",
        "from tqdm import tnrange, tqdm\n",
        "from collections import Counter\n",
        "from random import shuffle\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "from IPython.display import clear_output\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bFYy7uuHOpkk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ]
    },
    {
      "metadata": {
        "id": "pLFTZG20Opkm",
        "colab_type": "code",
        "outputId": "78e09223-91fc-4817-a5e1-8c77693bfe72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# some prep steps\n",
        "# for google colab\n",
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "  \n",
        "if IN_COLAB:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive/')\n",
        "  data_folder = \"/content/drive/My Drive/Colab Notebooks/data/\"\n",
        "else:\n",
        "  data_folder = \"data/\"\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "E4OYmCuGOpkq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#load .npz-file from folder\n",
        "\n",
        "\n",
        "\n",
        "loader = np.load(data_folder+\"humpback_300x100_gray_no_new.npz\")\n",
        "features = loader[\"features\"]\n",
        "labels = loader[\"labels\"]\n",
        "\n",
        "n_rows = labels.shape[0]\n",
        "\n",
        "\n",
        "\n",
        "split_ratio = 0.8\n",
        "data_size = 400\n",
        "expansion_factor = 1\n",
        "\n",
        "#take the sample from the most common classes\n",
        "ar = np.array(Counter(labels).most_common())\n",
        "count = 0\n",
        "label_list = []\n",
        "for idx, tup in enumerate(ar):\n",
        "    label_list.append(tup[0])\n",
        "    count += tup[1]\n",
        "    if count > data_size:\n",
        "        break\n",
        "label_list\n",
        "\n",
        "label_in_list=[x in label_list for x in labels]\n",
        "labels = labels[label_in_list]\n",
        "features = features[label_in_list]\n",
        "\n",
        "def standardize(X):\n",
        "    X = X.astype(np.float32)\n",
        "    X = (X - np.mean(X, axis=(1,2), keepdims=True)) / np.std(X, axis=(1,2), keepdims=True)\n",
        "    return X\n",
        "\n",
        "\n",
        "features = standardize(features)\n",
        "features = np.expand_dims(features, axis=3)\n",
        "\n",
        "# not needed\n",
        "#all_combinations_without_labels = np.array(list(itertools.combinations(range(data_size),2)))\n",
        "#same_label_indices = labels[all_combinations_without_labels[:,0]] == labels[all_combinations_without_labels[:,1]]\n",
        "#all_combinations = np.append(all_combinations_without_labels, same_label_indices.reshape(-1,1), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x0nialoEJH3d",
        "colab_type": "code",
        "outputId": "4bc4ffdb-61ea-4092-ee23-0f697f19e501",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "features.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(423, 100, 300, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "dko54Bt-Opku",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_test_split_old(split_ratio):\n",
        "    \"\"\"\n",
        "    Get unique combinations from [0..data_size].\n",
        "    Shuffle same_label_pairs and dif_label_pairs individually.\n",
        "    Sample the first split_ratio*num pairs of each class for training and \n",
        "    sample the the rest (1-split_ratio)*num pairs of each class for validation\n",
        "    \n",
        "    \"\"\"\n",
        "    same_label_pairs = np.random.permutation(all_combinations[same_label_indices])\n",
        "    dif_label_pairs = np.random.permutation(all_combinations[~same_label_indices])\n",
        "    \n",
        "    num_pairs = int(1/2*expansion_factor*data_size)\n",
        "    num_train = int(split_ratio*num_pairs)\n",
        "   \n",
        "    train_matchings = np.append(same_label_pairs[:num_train], dif_label_pairs[:num_train], axis = 0)\n",
        "    val_matchings = np.append(same_label_pairs[num_train:num_pairs], dif_label_pairs[num_train:num_pairs], axis = 0)\n",
        "    \n",
        "    np.random.shuffle(train_matchings), np.random.shuffle(val_matchings)\n",
        "    \n",
        "    return train_matchings.astype(int), val_matchings.astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z5hGTZT02xek",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_test_split_one_hot(split_ratio):\n",
        "    \"\"\"\n",
        "    Split data into train, validation and test set.\n",
        "    Test set contains just pictures. \n",
        "    Validation and Train set are pairs of pictures (unique matches).\n",
        "    \"\"\"\n",
        "    perm = np.random.permutation(range(data_size))\n",
        "    train_idx = perm[:int(split_ratio*data_size)]\n",
        "    test_idx = perm[int(split_ratio*data_size):]\n",
        "    \n",
        "    combinations = np.array(list(itertools.combinations(train_idx,2)))\n",
        "    \n",
        "    same_label_indices = labels[combinations[:,0]] == labels[combinations[:,1]]\n",
        "    combinations = np.append(np.append(combinations, same_label_indices.reshape(-1,1), axis=1), ~same_label_indices.reshape(-1,1), axis=1)\n",
        "\n",
        "    same_label_pairs = np.random.permutation(combinations[same_label_indices])\n",
        "    dif_label_pairs = np.random.permutation(combinations[~same_label_indices])\n",
        "    \n",
        "    num_pairs = int(1/2*expansion_factor*data_size)\n",
        "    num_train = int(split_ratio*num_pairs)\n",
        "   \n",
        "    train_matchings = np.append(same_label_pairs[:num_train], dif_label_pairs[:num_train], axis = 0)\n",
        "    val_matchings = np.append(same_label_pairs[num_train:num_pairs], dif_label_pairs[num_train:num_pairs], axis = 0)\n",
        "    \n",
        "    np.random.shuffle(train_matchings), np.random.shuffle(val_matchings)\n",
        "    return train_matchings.astype(int), val_matchings.astype(int), test_idx.astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SdRbxqisOpk1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_test_split(split_ratio):\n",
        "    \"\"\"\n",
        "    Split data into train, validation and test set.\n",
        "    Test set contains just pictures. \n",
        "    Validation and Train set are pairs of pictures (unique matches).\n",
        "    \"\"\"\n",
        "    perm = np.random.permutation(range(data_size))\n",
        "    train_idx = perm[:int(split_ratio*data_size)]\n",
        "    test_idx = perm[int(split_ratio*data_size):]\n",
        "    \n",
        "    combinations = np.array(list(itertools.combinations(train_idx,2)))\n",
        "    \n",
        "    same_label_indices = labels[combinations[:,0]] == labels[combinations[:,1]]\n",
        "    combinations = np.append(combinations, same_label_indices.reshape(-1,1), axis=1)\n",
        "\n",
        "    same_label_pairs = np.random.permutation(combinations[same_label_indices])\n",
        "    dif_label_pairs = np.random.permutation(combinations[~same_label_indices])\n",
        "    \n",
        "    num_pairs = int(1/2*expansion_factor*data_size)\n",
        "    num_train = int(split_ratio*num_pairs)\n",
        "   \n",
        "    train_matchings = np.append(same_label_pairs[:num_train], dif_label_pairs[:num_train], axis = 0)\n",
        "    val_matchings = np.append(same_label_pairs[num_train:num_pairs], dif_label_pairs[num_train:num_pairs], axis = 0)\n",
        "    \n",
        "    np.random.shuffle(train_matchings), np.random.shuffle(val_matchings)\n",
        "    return train_matchings.astype(int), val_matchings.astype(int), test_idx.astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V1sNvO_xOpk3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train Test Split"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "nMJQsDp1Opk4",
        "colab_type": "code",
        "outputId": "4bb2cc68-d70c-4bb7-9df1-9509f360b60b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_matchings, val_matchings, test_idx = train_test_split(split_ratio)\n",
        "train_matchings.shape, val_matchings.shape, test_idx.shape\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((320, 3), (80, 3), (80,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "3PCRBKf6Opk9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Helper functions for calculation of accuracy"
      ]
    },
    {
      "metadata": {
        "id": "Un1GL39342oE",
        "colab_type": "code",
        "outputId": "0e617ae2-57b4-42db-a93e-8c98c8281bd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "np.unique(labels)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 12,  18,  32,  58, 166, 241, 265])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "tyXLeFapOpk9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_unique_N(iterable, N):\n",
        "    \"\"\"Yields (in order) the first N unique elements of iterable. \n",
        "    Might yield less if data too short.\"\"\"\n",
        "    seen = set()\n",
        "    for e in iterable:\n",
        "        if e in seen: # ES IST NIE IN SEEN :D\n",
        "            continue\n",
        "        seen.add(e)\n",
        "        yield e\n",
        "        if len(seen) == N:\n",
        "            return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xewGMyTWOplC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_k_nearest(distance_matrix):\n",
        "    dm = distance_matrix.copy()\n",
        "    for i in range(dm.shape[1]):    \n",
        "        dm[i,i] += 100000 \n",
        "    top5_nearest = np.empty((distance_matrix.shape[0], 5))\n",
        "    for idx, line in enumerate(dm):\n",
        "        sorted_indices = np.argsort(line)\n",
        "        top5_nearest[idx,:] = np.fromiter(get_unique_N(labels[sorted_indices], 5), int)\n",
        "    #top5_nearest = labels[top5_nearest.astype(int)]\n",
        "    return top5_nearest.astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Bg6gRoytoUDn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "weights_standard = np.array([1, 0.8, 0.6, 0.4, 0.2])\n",
        "weights_first = np.array([1,0,0,0,0])\n",
        "weights_half = np.array([1,0.5,0.33,0.25,0.20])\n",
        "def calculate_accuracy_score(outputs_1, outputs_2=None, weights=weights_standard):\n",
        "    distance_matrix = euclidean_distances(outputs_1, outputs_1[test_idx])\n",
        "    top5_nearest = get_k_nearest(distance_matrix)\n",
        "    #print(top5_nearest)\n",
        "    true_labels = np.repeat(np.array([labels]), 5, axis=0).T\n",
        "    prediction_matrix = top5_nearest == true_labels\n",
        "    #print(np.count_nonzero(prediction_matrix))\n",
        "    for row in prediction_matrix:\n",
        "        for i, elt in enumerate(row):\n",
        "            if elt:\n",
        "                row[i+1:] = 0\n",
        "                break\n",
        "    #print(np.count_nonzero(prediction_matrix),\"/\",labels.shape[0],\"unique whales have correct labels in the top 5\")\n",
        "    scores_per_image = prediction_matrix@weights_half\n",
        "    score = np.sum(scores_per_image)/scores_per_image.shape[0]\n",
        "    return score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TbUWhUeHOplG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def batch_data(num_data, batch_size):\n",
        "    \"\"\" Yield batches with indices until epoch is over.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    num_data: int\n",
        "        The number of samples in the dataset.\n",
        "    batch_size: int\n",
        "        The batch size used using training.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    batch_ixs: np.array of ints with shape [batch_size,]\n",
        "        Yields arrays of indices of size of the batch size until the epoch is over.\n",
        "    \"\"\"\n",
        "    \n",
        "    data_ixs = np.random.permutation(np.arange(num_data))\n",
        "    ix = 0\n",
        "    while ix + batch_size < num_data:\n",
        "        batch_ixs = data_ixs[ix:ix+batch_size]\n",
        "        ix += batch_size\n",
        "        yield batch_ixs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MGLEBx2s4GsV",
        "colab_type": "code",
        "outputId": "686acaa0-bbf4-49c6-d3df-a381f6f23332",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "np.unique(labels)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 12,  18,  32,  58, 166, 241, 265])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "T1geGje-OplK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Check initializer of variables"
      ]
    },
    {
      "metadata": {
        "id": "ohoyQWFkOplL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SiamN:\n",
        "    \n",
        "    def __init__(self, name, learning_rate=0.001, height=100, width=300, channels=1, margin=0.5):\n",
        "        \n",
        "        self.name = name\n",
        "        self.dropout = tf.placeholder_with_default(0.0, shape=(), name=\"dropout\")\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weights =[]\n",
        "        self.biases =[]\n",
        "        self.margin = margin\n",
        "        \n",
        "        self.X1 = tf.placeholder(shape=[None, height, width, channels], dtype=tf.float32, name=\"data_1\") \n",
        "        self.X2 = tf.placeholder(shape=[None, height, width, channels], dtype=tf.float32, name=\"data_2\") \n",
        "        self.Y = tf.placeholder(shape=[None,], dtype=tf.float32, name=\"labels\") \n",
        "        \n",
        "        self.output1 = self.forward_pass(self.X1, reuse=False)\n",
        "        self.output2 = self.forward_pass(self.X2, reuse=True)\n",
        "        self.logits = self.head(self.output1, self.output2)\n",
        "        self.accuracy = tf.reduce_mean(tf.cast(tf.equal(self.logits > 0, tf.cast(self.Y, tf.bool)),tf.float32))\n",
        "        self.loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits, labels=self.Y, name=\"cross_entropy_loss\"))\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n",
        "        \n",
        "    \n",
        "    def forward_pass(self, X, reuse=False):\n",
        "        \n",
        "        convkwargs = {'padding': 'same', 'activation_fn': tf.nn.relu, 'reuse': reuse, 'weights_initializer': tf.contrib.layers.variance_scaling_initializer(dtype=tf.float32)}\n",
        "        poolkwargs = {'kernel_size': 2, 'stride': 2, 'padding': 'same'}\n",
        "        \n",
        "        with tf.variable_scope(\"conv1\") as scope:\n",
        "            x = tf.contrib.layers.conv2d(X, 64, kernel_size=9, stride=2, scope=scope, **convkwargs)\n",
        "            #x = tf.contrib.layers.batch_norm(x, scope=scope, reuse=reuse)\n",
        "            x = tf.contrib.layers.max_pool2d(x, **poolkwargs)\n",
        "        with tf.variable_scope(\"conv2\") as scope:\n",
        "            x = tf.contrib.layers.conv2d(x, 64, kernel_size=3, stride=1, scope=scope, **convkwargs)\n",
        "            #x = tf.contrib.layers.batch_norm(x, scope=scope, reuse=reuse)\n",
        "            x = tf.contrib.layers.max_pool2d(x, **poolkwargs)\n",
        "        with tf.variable_scope(\"conv3\") as scope:\n",
        "            x = tf.contrib.layers.conv2d(x, 128, kernel_size=3, stride=1, scope=scope, **convkwargs)\n",
        "            #x = tf.contrib.layers.batch_norm(x, scope=scope, reuse=reuse)\n",
        "            x = tf.contrib.layers.max_pool2d(x, **poolkwargs)\n",
        "        with tf.variable_scope(\"conv4\") as scope:\n",
        "            x = tf.contrib.layers.conv2d(x, 256, kernel_size=1, stride=1, scope=scope, **convkwargs)\n",
        "            #x = tf.contrib.layers.batch_norm(x, scope=scope, reuse=reuse)\n",
        "            x = tf.contrib.layers.max_pool2d(x, **poolkwargs)\n",
        "        \n",
        "        x = tf.contrib.layers.flatten(x)\n",
        "        x = tf.contrib.layers.fully_connected(x, 1000, activation_fn=tf.nn.relu, weights_initializer=tf.contrib.layers.variance_scaling_initializer(dtype=tf.float32))\n",
        "        #x = tf.contrib.layers.batch_norm(x)\n",
        "        \n",
        "        output = x\n",
        "            \n",
        "        return output\n",
        "    \n",
        "    def head(self, output1, output2): \n",
        "        x1 = tf.multiply(output1, output2)\n",
        "        x2 = tf.add(output1, output2)\n",
        "        x3 = tf.abs(output2 - output1)\n",
        "        x4 = tf.square(x3)\n",
        "        x = tf.concat([x1, x2, x3, x4], axis = 1)\n",
        "        \n",
        "        x = tf.reshape(x,[-1,4,x1.shape[1],1])\n",
        "        x = tf.contrib.layers.conv2d(x, 32, kernel_size=[4,1], activation_fn=tf.nn.relu, padding='valid')\n",
        "        x = tf.reshape(x, [-1,x1.shape[1], 32, 1])\n",
        "        x = tf.contrib.layers.conv2d(x, 1, kernel_size=[1,32], padding='valid')\n",
        "        \n",
        "        x = tf.contrib.layers.flatten(x)\n",
        "        #x = tf.contrib.layers.dropout(x)\n",
        "        print(x.shape)\n",
        "        x = tf.contrib.layers.fully_connected(x, 1)\n",
        "        print(x.shape)\n",
        "        return tf.reshape(x,[-1])\n",
        "\n",
        "    \n",
        "    def train(self, features, train_matchings, val_matchings, epochs=20, dropout=0.0, batch_size=512):\n",
        "\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        acc_scores = []\n",
        "        train_accs = []\n",
        "        val_accs = []\n",
        "        acc_scores.append(0)\n",
        "        \n",
        "        config = tf.ConfigProto()\n",
        "        #config.gpu_options.allow_growth=True\n",
        "        self.session = tf.Session(config=config)\n",
        "        session = self.session\n",
        "        \n",
        "        #tf.set_random_seed(0)\n",
        "        session.run(tf.global_variables_initializer())\n",
        "        \n",
        "        train_loss, train_acc = session.run([self.loss, self.accuracy], feed_dict={self.X1: features[train_matchings[:,0]], self.X2: features[train_matchings[:,1]], self.Y: train_matchings[:,2]})\n",
        "        val_loss, val_acc = session.run([self.loss, self.accuracy], feed_dict={self.X1: features[val_matchings[:,0]], self.X2: features[val_matchings[:,1]], self.Y: val_matchings[:,2]})\n",
        "\n",
        "        #output = session.run(self.output1, feed_dict={self.X1: features})\n",
        "        #acc_score = calculate_accuracy_score(output)\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        #acc_scores.append(int(acc_score*100))\n",
        "        train_accs.append(int(train_acc*100))\n",
        "        val_accs.append(int(val_acc*100))\n",
        "        \n",
        "        print(f\"Epoch 0 train_loss: {train_losses[-1]} val_loss: {val_losses[-1]} acc_score: {acc_scores[-1]} train_acc: {train_accs[-1]} val_acc: {val_accs[-1]}\")  \n",
        "        \n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            if (epoch+1) % 5 == 0:\n",
        "                print(f\"Epoch {epoch+1}/{epochs} train_loss: {train_losses[-1]} val_loss: {val_losses[-1]} acc_score: {acc_scores[-1]} train_acc: {train_accs[-1]} val_acc: {val_accs[-1]}\")  \n",
        "            for batch_ixs in batch_data(train_matchings.shape[0], batch_size):\n",
        "                    _ = session.run( self.optimizer, feed_dict={self.X1: features[train_matchings[batch_ixs,0]], self.X2: features[train_matchings[batch_ixs,1]], self.Y: train_matchings[batch_ixs,2]})  \n",
        "            \n",
        "            #TODO: remove boilerplate code, define function to calc accs and errors with flag print=TRUE/FALSE\n",
        "            train_loss, train_acc = session.run([self.loss, self.accuracy], feed_dict={self.X1: features[train_matchings[:,0]], self.X2: features[train_matchings[:,1]], self.Y: train_matchings[:,2]})\n",
        "            val_loss, val_acc = session.run([self.loss, self.accuracy], feed_dict={self.X1: features[val_matchings[:,0]], self.X2: features[val_matchings[:,1]], self.Y: val_matchings[:,2]})\n",
        "        \n",
        "            #output = session.run(self.output1, feed_dict={self.X1: features})\n",
        "            #acc_score = calculate_accuracy_score(output)\n",
        "\n",
        "            train_losses.append(train_loss)\n",
        "            val_losses.append(val_loss)\n",
        "            #acc_scores.append(int(acc_score*100))\n",
        "            train_accs.append(int(train_acc*100))\n",
        "            val_accs.append(int(val_acc*100))\n",
        "\n",
        "        \n",
        "        \n",
        "        self.hist={'train_loss': np.array(train_losses),\n",
        "           'val_loss': np.array(val_losses), \"epochs_trained\": epoch}\n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yv3NuyHD-8e-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#model1.session.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VGIBmg0cOplV",
        "colab_type": "code",
        "outputId": "e78189e6-e6b3-4d24-deef-bf03504f8d49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1357
        }
      },
      "cell_type": "code",
      "source": [
        "#You can change layer types and the number of neurons by changing the following variables.\n",
        "t = time.time()\n",
        "epochs = 400\n",
        "batch_size = 256\n",
        "\n",
        "tf.reset_default_graph()\n",
        "model1 = SiamN(\"first_model\", learning_rate = 0.0001, margin = 1)\n",
        "\n",
        "model1.train(features, train_matchings, val_matchings, epochs, batch_size=batch_size)\n",
        "print(\"Training finished in\", time.time()-t,\"s.\")\n",
        "\n",
        "\n",
        "#works: \n",
        "#head1layers and 5 \n",
        "#head1&2layers and 200 (0.0002) : 97 but then 50\n",
        "#\"\" ... and 500 (0.0002) : 98\n",
        "#he initializer made it better\n",
        "#\"\" ... and 1000(0.0001) : 100 after 70 epochs (9 out of 10 times its stuck at 50 50 though)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(?, 1000)\n",
            "(?, 1)\n",
            "Epoch 0 train_loss: 0.6934834122657776 val_loss: 0.6939243078231812 acc_score: 0 train_acc: 49 val_acc: 48\n",
            "Epoch 5/400 train_loss: 0.6820300817489624 val_loss: 0.6970582008361816 acc_score: 0 train_acc: 53 val_acc: 47\n",
            "Epoch 10/400 train_loss: 0.6598031520843506 val_loss: 0.6995150446891785 acc_score: 0 train_acc: 54 val_acc: 50\n",
            "Epoch 15/400 train_loss: 0.6321255564689636 val_loss: 0.7071720957756042 acc_score: 0 train_acc: 64 val_acc: 47\n",
            "Epoch 20/400 train_loss: 0.5903524160385132 val_loss: 0.7210813760757446 acc_score: 0 train_acc: 65 val_acc: 50\n",
            "Epoch 25/400 train_loss: 0.5397167205810547 val_loss: 0.7563114166259766 acc_score: 0 train_acc: 70 val_acc: 50\n",
            "Epoch 30/400 train_loss: 0.48033127188682556 val_loss: 0.7923809885978699 acc_score: 0 train_acc: 89 val_acc: 53\n",
            "Epoch 35/400 train_loss: 0.43941086530685425 val_loss: 0.8537197113037109 acc_score: 0 train_acc: 95 val_acc: 57\n",
            "Epoch 40/400 train_loss: 0.449535995721817 val_loss: 1.1093568801879883 acc_score: 0 train_acc: 71 val_acc: 51\n",
            "Epoch 45/400 train_loss: 0.4129787087440491 val_loss: 0.9190662503242493 acc_score: 0 train_acc: 99 val_acc: 53\n",
            "Epoch 50/400 train_loss: 0.3837684690952301 val_loss: 1.0002342462539673 acc_score: 0 train_acc: 98 val_acc: 55\n",
            "Epoch 55/400 train_loss: 0.3624798655509949 val_loss: 1.192090630531311 acc_score: 0 train_acc: 98 val_acc: 55\n",
            "Epoch 60/400 train_loss: 0.35693690180778503 val_loss: 1.2798669338226318 acc_score: 0 train_acc: 98 val_acc: 56\n",
            "Epoch 65/400 train_loss: 0.3520570397377014 val_loss: 1.3905233144760132 acc_score: 0 train_acc: 99 val_acc: 57\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-7fbf05c61777>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSiamN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"first_model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmargin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_matchings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_matchings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training finished in\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"s.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-7201af9cb875>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, features, train_matchings, val_matchings, epochs, dropout, batch_size)\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{epochs} train_loss: {train_losses[-1]} val_loss: {val_losses[-1]} acc_score: {acc_scores[-1]} train_acc: {train_accs[-1]} val_acc: {val_accs[-1]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch_ixs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_matchings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_matchings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_ixs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_matchings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_ixs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_matchings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_ixs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;31m#TODO: remove boilerplate code, define function to calc accs and errors with flag print=TRUE/FALSE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "MQ8Lv095xC3r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test = tf.constant([[1,2,3], [1,2,3]])\n",
        "test.shape.ndims"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8yXNZD__vwdA",
        "colab_type": "code",
        "outputId": "7afb5d85-07cc-4e41-f8dc-0f9fbce1d991",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "import pdb; pdb.pm()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py(861)_get_single_variable()\n",
            "-> name, \"\".join(traceback.format_list(tb))))\n",
            "(Pdb) model1.X1\n",
            "*** NameError: name 'model1' is not defined\n",
            "(Pdb) quit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eJDSOUmfzXTW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "scores_1, scores_2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LyOnBVXNOpla",
        "colab_type": "code",
        "outputId": "b3ee0e98-d4d4-451c-cc1c-86c5b85e89f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "cell_type": "code",
      "source": [
        "calculate_accuracy_score(outputs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-122-2cdb18d21b11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcalculate_accuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'outputs' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "97A_8CIdOpl5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XA0UdlPDOpl-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "idx = np.random.randint(3200)\n",
        "pic_a = features[train_matchings[idx,0]]\n",
        "pic_b = features[train_matchings[idx,1]]\n",
        "print(f\"same whales? {bool(train_matchings[idx,2])}\")    \n",
        "print(\"Index: \", idx)\n",
        "distance = model1.session.run(model1.distance, feed_dict={model1.X_1: np.array([pic_a]), model1.X_2: np.array([pic_b]), model1.Y: np.array([train_matchings[idx,2]])})\n",
        "print(f\"distance: {distance}\")\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.imshow(pic_a[0], cmap=\"gray\")\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.imshow(pic_b[0], cmap=\"gray\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c3hVYFY6OpmA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"model 1\")\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(model1.hist['train_loss'][5::], label=\"Training\")\n",
        "plt.plot(model1.hist['val_loss'][5::], label=\"Validation\")\n",
        "\n",
        "plt.xlabel(\"Epoch\", fontsize=20)\n",
        "plt.ylabel(\"Loss\", fontsize=20)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qafg5OErOpmE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(model1.hist[\"train_loss\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4Rb7k5OAOpmI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Experimental: tensorflow datasets"
      ]
    },
    {
      "metadata": {
        "id": "4a7zPSbuOpmK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_data = tf.data.Dataset.from_tensor_slices({\"feature\": train_data, \"label\": train_labels})\n",
        "val_data = tf.data.Dataset.from_tensor_slices({\"feature\": val_data, \"label\": val_labels})\n",
        "train_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jXWGdlTBOpmN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_data.output_types"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gmsCTYwuOpmQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#build batches\n",
        "batch_size = 500\n",
        "train_data.shuffle(30000)\n",
        "batches = dataset.batch(batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O_zqsyoIOpmV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "sess = tf.Session()\n",
        "iterator = batches.make_one_shot_iterator()\n",
        "next_element = iterator.get_next()\n",
        "no_of_batches = int(np.ceil(labels.shape[0] / batch_size))\n",
        "counter = 1\n",
        "for i in tqdm(range(no_of_batches)):\n",
        "    value = sess.run(next_element)\n",
        "    print(value[\"feature\"].shape)\n",
        "    print(counter)\n",
        "    counter+=1\n",
        "\n",
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rG-K_AbzQmM3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# run this to check available gpu memory (i got 5gb)\n",
        "\n",
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}