{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab_siamese_networks.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Beno71/humpback-whale-classification/blob/master/Colab_siamese_network_Cross_Entropy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "caf9Pin0ddx6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os \n",
        "print(os.getcwd())\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KPOZ_GvIOpkR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# siamese networks for whale classification"
      ]
    },
    {
      "metadata": {
        "id": "miwpXH5NOpkX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.rendered_html { font-size: 18px; }</style>\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "swbuAkt8Opkg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import os, time, itertools\n",
        "from skimage import io, transform\n",
        "import skimage\n",
        "import glob\n",
        "from tqdm import tnrange, tqdm\n",
        "from collections import Counter\n",
        "from random import shuffle\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "from IPython.display import clear_output\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bFYy7uuHOpkk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ]
    },
    {
      "metadata": {
        "id": "pLFTZG20Opkm",
        "colab_type": "code",
        "outputId": "b10a76f6-d211-4c7c-a162-55712a3a86b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "# some prep steps\n",
        "# for google colab\n",
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "  \n",
        "if IN_COLAB:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive/')\n",
        "  data_folder = \"/content/drive/My Drive/Colab Notebooks/data/\"\n",
        "else:\n",
        "  data_folder = \"data/\"\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "E4OYmCuGOpkq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#load .npz-file from folder\n",
        "\n",
        "\n",
        "\n",
        "loader = np.load(data_folder+\"humpback_300x100_gray_no_new.npz\")\n",
        "features = loader[\"features\"]\n",
        "labels = loader[\"labels\"]\n",
        "\n",
        "n_rows = labels.shape[0]\n",
        "\n",
        "\n",
        "\n",
        "split_ratio = 0.8\n",
        "data_size = 400\n",
        "expansion_factor = 1\n",
        "\n",
        "#take the sample from the most common classes\n",
        "ar = np.array(Counter(labels).most_common())\n",
        "count = 0\n",
        "label_list = []\n",
        "for idx, tup in enumerate(ar):\n",
        "    label_list.append(tup[0])\n",
        "    count += tup[1]\n",
        "    if count > data_size:\n",
        "        break\n",
        "label_list\n",
        "\n",
        "label_in_list=[x in label_list for x in labels]\n",
        "labels = labels[label_in_list]\n",
        "features = features[label_in_list]\n",
        "\n",
        "def standardize(X):\n",
        "    X = X.astype(np.float32)\n",
        "    X = (X - np.mean(X, axis=(1,2), keepdims=True)) / np.std(X, axis=(1,2), keepdims=True)\n",
        "    return X\n",
        "\n",
        "features = standardize(features)\n",
        "features = np.expand_dims(features, axis=1)\n",
        "\n",
        "# not needed\n",
        "#all_combinations_without_labels = np.array(list(itertools.combinations(range(data_size),2)))\n",
        "#same_label_indices = labels[all_combinations_without_labels[:,0]] == labels[all_combinations_without_labels[:,1]]\n",
        "#all_combinations = np.append(all_combinations_without_labels, same_label_indices.reshape(-1,1), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x0nialoEJH3d",
        "colab_type": "code",
        "outputId": "f62f26fc-a09c-48f5-e94a-d352fc65ace2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "np.unique(labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 12,  18,  32,  58, 166, 241, 265])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "metadata": {
        "id": "dko54Bt-Opku",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_test_split_old(split_ratio):\n",
        "    \"\"\"\n",
        "    Get unique combinations from [0..data_size].\n",
        "    Shuffle same_label_pairs and dif_label_pairs individually.\n",
        "    Sample the first split_ratio*num pairs of each class for training and \n",
        "    sample the the rest (1-split_ratio)*num pairs of each class for validation\n",
        "    \n",
        "    \"\"\"\n",
        "    same_label_pairs = np.random.permutation(all_combinations[same_label_indices])\n",
        "    dif_label_pairs = np.random.permutation(all_combinations[~same_label_indices])\n",
        "    \n",
        "    num_pairs = int(1/2*expansion_factor*data_size)\n",
        "    num_train = int(split_ratio*num_pairs)\n",
        "   \n",
        "    train_matchings = np.append(same_label_pairs[:num_train], dif_label_pairs[:num_train], axis = 0)\n",
        "    val_matchings = np.append(same_label_pairs[num_train:num_pairs], dif_label_pairs[num_train:num_pairs], axis = 0)\n",
        "    \n",
        "    np.random.shuffle(train_matchings), np.random.shuffle(val_matchings)\n",
        "    \n",
        "    return train_matchings.astype(int), val_matchings.astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z5hGTZT02xek",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_test_split_one_hot(split_ratio):\n",
        "    \"\"\"\n",
        "    Split data into train, validation and test set.\n",
        "    Test set contains just pictures. \n",
        "    Validation and Train set are pairs of pictures (unique matches).\n",
        "    \"\"\"\n",
        "    perm = np.random.permutation(range(data_size))\n",
        "    train_idx = perm[:int(split_ratio*data_size)]\n",
        "    test_idx = perm[int(split_ratio*data_size):]\n",
        "    \n",
        "    combinations = np.array(list(itertools.combinations(train_idx,2)))\n",
        "    \n",
        "    same_label_indices = labels[combinations[:,0]] == labels[combinations[:,1]]\n",
        "    combinations = np.append(np.append(combinations, same_label_indices.reshape(-1,1), axis=1), ~same_label_indices.reshape(-1,1), axis=1)\n",
        "\n",
        "    same_label_pairs = np.random.permutation(combinations[same_label_indices])\n",
        "    dif_label_pairs = np.random.permutation(combinations[~same_label_indices])\n",
        "    \n",
        "    num_pairs = int(1/2*expansion_factor*data_size)\n",
        "    num_train = int(split_ratio*num_pairs)\n",
        "   \n",
        "    train_matchings = np.append(same_label_pairs[:num_train], dif_label_pairs[:num_train], axis = 0)\n",
        "    val_matchings = np.append(same_label_pairs[num_train:num_pairs], dif_label_pairs[num_train:num_pairs], axis = 0)\n",
        "    \n",
        "    np.random.shuffle(train_matchings), np.random.shuffle(val_matchings)\n",
        "    return train_matchings.astype(int), val_matchings.astype(int), test_idx.astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SdRbxqisOpk1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_test_split(split_ratio):\n",
        "    \"\"\"\n",
        "    Split data into train, validation and test set.\n",
        "    Test set contains just pictures. \n",
        "    Validation and Train set are pairs of pictures (unique matches).\n",
        "    \"\"\"\n",
        "    perm = np.random.permutation(range(data_size))\n",
        "    train_idx = perm[:int(split_ratio*data_size)]\n",
        "    test_idx = perm[int(split_ratio*data_size):]\n",
        "    \n",
        "    combinations = np.array(list(itertools.combinations(train_idx,2)))\n",
        "    \n",
        "    same_label_indices = labels[combinations[:,0]] == labels[combinations[:,1]]\n",
        "    combinations = np.append(combinations, same_label_indices.reshape(-1,1), axis=1)\n",
        "\n",
        "    same_label_pairs = np.random.permutation(combinations[same_label_indices])\n",
        "    dif_label_pairs = np.random.permutation(combinations[~same_label_indices])\n",
        "    \n",
        "    num_pairs = int(1/2*expansion_factor*data_size)\n",
        "    num_train = int(split_ratio*num_pairs)\n",
        "   \n",
        "    train_matchings = np.append(same_label_pairs[:num_train], dif_label_pairs[:num_train], axis = 0)\n",
        "    val_matchings = np.append(same_label_pairs[num_train:num_pairs], dif_label_pairs[num_train:num_pairs], axis = 0)\n",
        "    \n",
        "    np.random.shuffle(train_matchings), np.random.shuffle(val_matchings)\n",
        "    return train_matchings.astype(int), val_matchings.astype(int), test_idx.astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V1sNvO_xOpk3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train Test Split"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "nMJQsDp1Opk4",
        "colab_type": "code",
        "outputId": "c20aea1e-572c-4d76-8f35-1dde5d30a3a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_matchings, val_matchings, test_idx = train_test_split(split_ratio)\n",
        "train_matchings.shape, val_matchings.shape, test_idx.shape\n"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((320, 3), (80, 3), (80,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "metadata": {
        "id": "3PCRBKf6Opk9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Helper functions for calculation of accuracy"
      ]
    },
    {
      "metadata": {
        "id": "Un1GL39342oE",
        "colab_type": "code",
        "outputId": "54031f1a-807d-48c6-c041-de351e6cf518",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "a = np.array([1,1,2,1,4,5,6,7])\n",
        "np.argsort(a)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 3, 2, 4, 5, 6, 7])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "tyXLeFapOpk9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_unique_N(iterable, N):\n",
        "    \"\"\"Yields (in order) the first N unique elements of iterable. \n",
        "    Might yield less if data too short.\"\"\"\n",
        "    seen = set()\n",
        "    for e in iterable:\n",
        "        if e in seen: # ES IST NIE IN SEEN :D\n",
        "            continue\n",
        "        seen.add(e)\n",
        "        yield e\n",
        "        if len(seen) == N:\n",
        "            return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xewGMyTWOplC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_k_nearest(distance_matrix):\n",
        "    dm = distance_matrix.copy()\n",
        "    for i in range(dm.shape[1]):    \n",
        "        dm[i,i] += 100000 \n",
        "    top5_nearest = np.empty((distance_matrix.shape[0], 5))\n",
        "    for idx, line in enumerate(dm):\n",
        "        sorted_indices = np.argsort(line)\n",
        "        top5_nearest[idx,:] = np.fromiter(get_unique_N(labels[sorted_indices], 5), int)\n",
        "    #top5_nearest = labels[top5_nearest.astype(int)]\n",
        "    return top5_nearest.astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Bg6gRoytoUDn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "weights_standard = np.array([1, 0.8, 0.6, 0.4, 0.2])\n",
        "weights_first = np.array([1,0,0,0,0])\n",
        "weights_half = np.array([1,0.5,0.33,0.25,0.20])\n",
        "def calculate_accuracy_score(outputs_1, outputs_2=None, weights=weights_standard):\n",
        "    distance_matrix = euclidean_distances(outputs_1, outputs_1[test_idx])\n",
        "    top5_nearest = get_k_nearest(distance_matrix)\n",
        "    #print(top5_nearest)\n",
        "    true_labels = np.repeat(np.array([labels]), 5, axis=0).T\n",
        "    prediction_matrix = top5_nearest == true_labels\n",
        "    #print(np.count_nonzero(prediction_matrix))\n",
        "    for row in prediction_matrix:\n",
        "        for i, elt in enumerate(row):\n",
        "            if elt:\n",
        "                row[i+1:] = 0\n",
        "                break\n",
        "    #print(np.count_nonzero(prediction_matrix),\"/\",labels.shape[0],\"unique whales have correct labels in the top 5\")\n",
        "    scores_per_image = prediction_matrix@weights_half\n",
        "    score = np.sum(scores_per_image)/scores_per_image.shape[0]\n",
        "    return score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TbUWhUeHOplG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def batch_data(num_data, batch_size):\n",
        "    \"\"\" Yield batches with indices until epoch is over.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    num_data: int\n",
        "        The number of samples in the dataset.\n",
        "    batch_size: int\n",
        "        The batch size used using training.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    batch_ixs: np.array of ints with shape [batch_size,]\n",
        "        Yields arrays of indices of size of the batch size until the epoch is over.\n",
        "    \"\"\"\n",
        "    \n",
        "    data_ixs = np.random.permutation(np.arange(num_data))\n",
        "    ix = 0\n",
        "    while ix + batch_size < num_data:\n",
        "        batch_ixs = data_ixs[ix:ix+batch_size]\n",
        "        ix += batch_size\n",
        "        yield batch_ixs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MGLEBx2s4GsV",
        "colab_type": "code",
        "outputId": "6a678a78-76c3-4c1c-b382-a1676e14153c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "a = np.arange(20).reshape((4,-1))\n",
        "a\n"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  1,  2,  3,  4],\n",
              "       [ 5,  6,  7,  8,  9],\n",
              "       [10, 11, 12, 13, 14],\n",
              "       [15, 16, 17, 18, 19]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "metadata": {
        "id": "T1geGje-OplK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Check initializer of variables"
      ]
    },
    {
      "metadata": {
        "id": "ohoyQWFkOplL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SiamN:\n",
        "    \n",
        "    def __init__(self, name, learning_rate=0.001, length=300, height=100, channels=1, margin=0.5):\n",
        "        \n",
        "        self.name = name\n",
        "        self.dropout = tf.placeholder_with_default(0.0, shape=(), name=\"dropout\")\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weights =[]\n",
        "        self.biases =[]\n",
        "        self.margin = margin\n",
        "        \n",
        "        self.X1 = tf.placeholder(shape=[None, channels, height, length], dtype=tf.float32, name=\"data_1\") \n",
        "        self.X2 = tf.placeholder(shape=[None, channels, height, length], dtype=tf.float32, name=\"data_2\") \n",
        "        self.Y = tf.placeholder(shape=[None,], dtype=tf.float32, name=\"labels\") \n",
        "        \n",
        "        self.output1 = self.forward_pass(self.X1, reuse=False)\n",
        "        self.output2 = self.forward_pass(self.X2, reuse=True)\n",
        "        self.logits = self.head(self.output1, self.output2)\n",
        "        self.accuracy = tf.reduce_mean(tf.cast(tf.equal(self.logits > 0, tf.cast(self.Y, tf.bool)),tf.float32))\n",
        "        self.loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits, labels=self.Y, name=\"cross_entropy_loss\"))\n",
        "        #self.loss = tf.losses.softmax_cross_entropy(self.Y, self.logits)\n",
        "        #self.loss = tf.contrib.losses.metric_learning.contrastive_loss(self.Y, self.output1, self.output2, self.margin)\n",
        "        #self.loss = self.contrastive_loss(self.Y, self.output1, self.output2, self.margin)\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n",
        "        \n",
        "    \n",
        "    def forward_pass(self, X, reuse=False):\n",
        "        \n",
        "        convkwargs = {'padding': 'same', 'activation_fn': tf.nn.relu, 'reuse': reuse}\n",
        "        poolkwargs = {'kernel_size': 2, 'stride': 2, 'padding': 'same'}\n",
        "        \n",
        "        with tf.variable_scope(\"conv1\") as scope:\n",
        "            x = tf.contrib.layers.conv2d(X, 32, kernel_size=7, stride=2, scope=scope, **convkwargs)\n",
        "            #x = tf.contrib.layers.batch_norm(x, scope=scope, reuse=reuse)\n",
        "            x = tf.contrib.layers.max_pool2d(x, **poolkwargs)\n",
        "        with tf.variable_scope(\"conv2\") as scope:\n",
        "            x = tf.contrib.layers.conv2d(x, 64, kernel_size=7, stride=1, scope=scope, **convkwargs)\n",
        "            #x = tf.contrib.layers.batch_norm(x, scope=scope, reuse=reuse)\n",
        "            x = tf.contrib.layers.max_pool2d(x, **poolkwargs)\n",
        "        with tf.variable_scope(\"conv3\") as scope:\n",
        "            x = tf.contrib.layers.conv2d(x, 128, kernel_size=5, stride=1, scope=scope, **convkwargs)\n",
        "            #x = tf.contrib.layers.batch_norm(x, scope=scope, reuse=reuse)\n",
        "            x = tf.contrib.layers.max_pool2d(x, **poolkwargs)\n",
        "        with tf.variable_scope(\"conv4\") as scope:\n",
        "            x = tf.contrib.layers.conv2d(x, 256, kernel_size=3, stride=1, scope=scope, **convkwargs)\n",
        "            #x = tf.contrib.layers.batch_norm(x, scope=scope, reuse=reuse)\n",
        "            x = tf.contrib.layers.max_pool2d(x, **poolkwargs)\n",
        "                           \n",
        "        \n",
        "        x = tf.contrib.layers.flatten(x)\n",
        "        x = tf.contrib.layers.fully_connected(x, 20)\n",
        "        #x = tf.contrib.layers.batch_norm(x)\n",
        "        \n",
        "        output = x\n",
        "            \n",
        "        return output\n",
        "    \n",
        "    def head(self, output1, output2): \n",
        "        x1 = tf.multiply(output1, output2)\n",
        "        x2 = tf.add(output1, output2)\n",
        "        x3 = tf.abs(output2 - output1)\n",
        "        x4 = tf.square(x3)\n",
        "        x = tf.concat([x1, x2, x3, x4], axis = 1)\n",
        "        #x = tf.reshape(x,[-1,4,x1.shape[1],1])\n",
        "        #x = tf.contrib.layers.conv2d(x, 32, kernel_size=[4,1], activation_fn=tf.nn.relu, padding='valid')\n",
        "        x = tf.reshape(x,[-1,1,4,x1.shape[1]])\n",
        "        x = tf.contrib.layers.conv2d(x, 32, kernel_size=[4,1], activation_fn=tf.nn.relu, padding='same')\n",
        "        print(x.shape)\n",
        "        #x = tf.reshape(x, [-1,x1.shape[1], 32, 1])\n",
        "        print(x.shape)\n",
        "        #x = tf.contrib.layers.conv2d(x, 1, kernel_size=[1,32], padding='valid')\n",
        "        \n",
        "        print(x.shape)\n",
        "        x = tf.contrib.layers.flatten(x)\n",
        "        print(x.shape)\n",
        "        x = tf.contrib.layers.fully_connected(x, 1)\n",
        "        print(x.shape)\n",
        "        return tf.reshape(x,[-1])\n",
        "\n",
        "    \n",
        "    def train(self, features, train_matchings, val_matchings, epochs=20, dropout=0.0, batch_size=512):\n",
        "\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        acc_scores = []\n",
        "        train_accs = []\n",
        "        val_accs = []\n",
        "        \n",
        "        config = tf.ConfigProto()\n",
        "        #config.gpu_options.allow_growth=True\n",
        "        self.session = tf.Session(config=config)\n",
        "        session = self.session\n",
        "        \n",
        "        session.run(tf.global_variables_initializer())\n",
        "        \n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            if (epoch+1) % 5 == 0:\n",
        "                print(f\"Epoch {epoch+1}/{epochs} train_loss: {train_losses[-1]} val_loss: {val_losses[-1]} acc_score: {acc_scores[-1]} train_acc: {train_accs[-1]} val_acc: {val_accs[-1]}\")  \n",
        "            for batch_ixs in batch_data(train_matchings.shape[0], batch_size):\n",
        "                    _ = session.run( self.optimizer, feed_dict={self.X1: features[train_matchings[batch_ixs,0]], self.X2: features[train_matchings[batch_ixs,1]], self.Y: train_matchings[batch_ixs,2]})  \n",
        "            \n",
        "            #TODO: remove boilerplate code, define function to calc accs and errors with flag print=TRUE/FALSE\n",
        "            train_loss, train_acc = session.run([self.loss, self.accuracy], feed_dict={self.X1: features[train_matchings[:,0]], self.X2: features[train_matchings[:,1]], self.Y: train_matchings[:,2]})\n",
        "            val_loss, val_acc = session.run([self.loss, self.accuracy], feed_dict={self.X1: features[val_matchings[:,0]], self.X2: features[val_matchings[:,1]], self.Y: val_matchings[:,2]})\n",
        "        \n",
        "            output = session.run(self.output1, feed_dict={self.X1: features})\n",
        "            acc_score = calculate_accuracy_score(output)\n",
        "\n",
        "            train_losses.append(train_loss)\n",
        "            val_losses.append(val_loss)\n",
        "            acc_scores.append(int(acc_score*100))\n",
        "            train_accs.append(int(train_acc*100))\n",
        "            val_accs.append(int(val_acc*100))\n",
        "\n",
        "        \n",
        "        \n",
        "        self.hist={'train_loss': np.array(train_losses),\n",
        "           'val_loss': np.array(val_losses), \"epochs_trained\": epoch}\n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yv3NuyHD-8e-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#model1.session.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VGIBmg0cOplV",
        "colab_type": "code",
        "outputId": "53fd5744-de7f-4be2-cb97-10ce32080e75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "cell_type": "code",
      "source": [
        "#You can change layer types and the number of neurons by changing the following variables.\n",
        "t = time.time()\n",
        "epochs = 200\n",
        "batch_size = 256\n",
        "\n",
        "tf.reset_default_graph()\n",
        "model1 = SiamN(\"first_model\", learning_rate = 0.001, margin = 1)\n",
        "\n",
        "model1.train(features, train_matchings, val_matchings, epochs, batch_size=batch_size)\n",
        "print(\"Training finished in\", time.time()-t,\"s.\")\n",
        "\n",
        "\n",
        "#shape of tuple can not be built\n",
        "# in particular:\n",
        "# features[train_matchings[:,0]].shape.ndims\n",
        "# probably should be tensor instead of np.array"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(?, 1, 4, 32)\n",
            "(?, 1, 4, 32)\n",
            "(?, 1, 4, 32)\n",
            "(?, 128)\n",
            "(?, 1)\n",
            "Epoch 5/200 train_loss: 0.690855860710144 val_loss: 0.6938424110412598 acc_score: 32 train_acc: 52 val_acc: 47\n",
            "Epoch 10/200 train_loss: 0.6663960814476013 val_loss: 0.6931777596473694 acc_score: 31 train_acc: 63 val_acc: 50\n",
            "Epoch 15/200 train_loss: 0.6382750868797302 val_loss: 0.7079925537109375 acc_score: 34 train_acc: 62 val_acc: 52\n",
            "Epoch 20/200 train_loss: 0.6060922741889954 val_loss: 0.7138959169387817 acc_score: 34 train_acc: 72 val_acc: 55\n",
            "Epoch 25/200 train_loss: 0.5809082984924316 val_loss: 0.7055756449699402 acc_score: 32 train_acc: 76 val_acc: 63\n",
            "Epoch 30/200 train_loss: 0.5497311353683472 val_loss: 0.8388239741325378 acc_score: 31 train_acc: 73 val_acc: 52\n",
            "Epoch 35/200 train_loss: 0.5063486099243164 val_loss: 0.8699529767036438 acc_score: 32 train_acc: 79 val_acc: 55\n",
            "Epoch 40/200 train_loss: 0.4906540513038635 val_loss: 1.0573160648345947 acc_score: 32 train_acc: 78 val_acc: 55\n",
            "Epoch 45/200 train_loss: 0.49965590238571167 val_loss: 0.9697425961494446 acc_score: 31 train_acc: 80 val_acc: 58\n",
            "Epoch 50/200 train_loss: 0.4792916774749756 val_loss: 1.1175050735473633 acc_score: 31 train_acc: 82 val_acc: 56\n",
            "Epoch 55/200 train_loss: 0.46713829040527344 val_loss: 1.4491112232208252 acc_score: 30 train_acc: 82 val_acc: 56\n",
            "Epoch 60/200 train_loss: 0.46656036376953125 val_loss: 1.2062989473342896 acc_score: 31 train_acc: 83 val_acc: 58\n",
            "Epoch 65/200 train_loss: 0.48105889558792114 val_loss: 1.314138650894165 acc_score: 32 train_acc: 80 val_acc: 55\n",
            "Epoch 70/200 train_loss: 0.46242719888687134 val_loss: 1.3611392974853516 acc_score: 30 train_acc: 83 val_acc: 58\n",
            "Epoch 75/200 train_loss: 0.46679893136024475 val_loss: 1.0757901668548584 acc_score: 30 train_acc: 83 val_acc: 57\n",
            "Epoch 80/200 train_loss: 0.45973873138427734 val_loss: 1.1295307874679565 acc_score: 29 train_acc: 83 val_acc: 55\n",
            "Epoch 85/200 train_loss: 0.45658913254737854 val_loss: 1.5110623836517334 acc_score: 28 train_acc: 84 val_acc: 52\n",
            "Epoch 90/200 train_loss: 0.45877400040626526 val_loss: 1.1226558685302734 acc_score: 29 train_acc: 84 val_acc: 56\n",
            "Epoch 95/200 train_loss: 0.4569348394870758 val_loss: 1.3307958841323853 acc_score: 29 train_acc: 84 val_acc: 55\n",
            "Epoch 100/200 train_loss: 0.4555397033691406 val_loss: 1.5799434185028076 acc_score: 30 train_acc: 84 val_acc: 55\n",
            "Epoch 105/200 train_loss: 0.48331061005592346 val_loss: 1.8096752166748047 acc_score: 30 train_acc: 83 val_acc: 56\n",
            "Epoch 110/200 train_loss: 0.47954702377319336 val_loss: 1.2937381267547607 acc_score: 30 train_acc: 81 val_acc: 58\n",
            "Epoch 115/200 train_loss: 0.47016072273254395 val_loss: 1.1504409313201904 acc_score: 27 train_acc: 82 val_acc: 56\n",
            "Epoch 120/200 train_loss: 0.4869046211242676 val_loss: 1.5517686605453491 acc_score: 31 train_acc: 81 val_acc: 56\n",
            "Epoch 125/200 train_loss: 0.46322041749954224 val_loss: 1.1708571910858154 acc_score: 31 train_acc: 83 val_acc: 60\n",
            "Epoch 130/200 train_loss: 0.46258658170700073 val_loss: 1.1188901662826538 acc_score: 31 train_acc: 83 val_acc: 57\n",
            "Epoch 135/200 train_loss: 0.4603891968727112 val_loss: 1.3110758066177368 acc_score: 31 train_acc: 83 val_acc: 58\n",
            "Epoch 140/200 train_loss: 0.4575914442539215 val_loss: 1.2203792333602905 acc_score: 30 train_acc: 84 val_acc: 58\n",
            "Epoch 145/200 train_loss: 0.4574955999851227 val_loss: 1.2353124618530273 acc_score: 31 train_acc: 84 val_acc: 61\n",
            "Epoch 150/200 train_loss: 0.4571775794029236 val_loss: 1.451722264289856 acc_score: 30 train_acc: 83 val_acc: 62\n",
            "Epoch 155/200 train_loss: 0.4572142958641052 val_loss: 1.4616174697875977 acc_score: 31 train_acc: 84 val_acc: 62\n",
            "Epoch 160/200 train_loss: 0.4597679674625397 val_loss: 1.3093907833099365 acc_score: 31 train_acc: 83 val_acc: 60\n",
            "Epoch 165/200 train_loss: 0.4595816135406494 val_loss: 1.5041449069976807 acc_score: 30 train_acc: 84 val_acc: 57\n",
            "Epoch 170/200 train_loss: 0.4630623459815979 val_loss: 1.5618536472320557 acc_score: 32 train_acc: 83 val_acc: 60\n",
            "Epoch 175/200 train_loss: 0.4617553651332855 val_loss: 1.0441759824752808 acc_score: 32 train_acc: 84 val_acc: 61\n",
            "Epoch 180/200 train_loss: 0.46673399209976196 val_loss: 0.9682121276855469 acc_score: 29 train_acc: 83 val_acc: 57\n",
            "Epoch 185/200 train_loss: 0.458047479391098 val_loss: 1.3671352863311768 acc_score: 32 train_acc: 84 val_acc: 55\n",
            "Epoch 190/200 train_loss: 0.4561897814273834 val_loss: 1.5783002376556396 acc_score: 29 train_acc: 84 val_acc: 58\n",
            "Epoch 195/200 train_loss: 0.4752185344696045 val_loss: 2.533867359161377 acc_score: 30 train_acc: 84 val_acc: 60\n",
            "Epoch 200/200 train_loss: 0.45524945855140686 val_loss: 1.5168486833572388 acc_score: 29 train_acc: 84 val_acc: 60\n",
            "Training finished in 51.88627052307129 s.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MQ8Lv095xC3r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test = tf.constant([[1,2,3], [1,2,3]])\n",
        "test.shape.ndims"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8yXNZD__vwdA",
        "colab_type": "code",
        "outputId": "7afb5d85-07cc-4e41-f8dc-0f9fbce1d991",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "import pdb; pdb.pm()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py(861)_get_single_variable()\n",
            "-> name, \"\".join(traceback.format_list(tb))))\n",
            "(Pdb) model1.X1\n",
            "*** NameError: name 'model1' is not defined\n",
            "(Pdb) quit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eJDSOUmfzXTW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "scores_1, scores_2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LyOnBVXNOpla",
        "colab_type": "code",
        "outputId": "b3ee0e98-d4d4-451c-cc1c-86c5b85e89f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "cell_type": "code",
      "source": [
        "calculate_accuracy_score(outputs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-122-2cdb18d21b11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcalculate_accuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'outputs' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "97A_8CIdOpl5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XA0UdlPDOpl-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "idx = np.random.randint(3200)\n",
        "pic_a = features[train_matchings[idx,0]]\n",
        "pic_b = features[train_matchings[idx,1]]\n",
        "print(f\"same whales? {bool(train_matchings[idx,2])}\")    \n",
        "print(\"Index: \", idx)\n",
        "distance = model1.session.run(model1.distance, feed_dict={model1.X_1: np.array([pic_a]), model1.X_2: np.array([pic_b]), model1.Y: np.array([train_matchings[idx,2]])})\n",
        "print(f\"distance: {distance}\")\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.imshow(pic_a[0], cmap=\"gray\")\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.imshow(pic_b[0], cmap=\"gray\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c3hVYFY6OpmA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"model 1\")\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(model1.hist['train_loss'][5::], label=\"Training\")\n",
        "plt.plot(model1.hist['val_loss'][5::], label=\"Validation\")\n",
        "\n",
        "plt.xlabel(\"Epoch\", fontsize=20)\n",
        "plt.ylabel(\"Loss\", fontsize=20)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qafg5OErOpmE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(model1.hist[\"train_loss\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4Rb7k5OAOpmI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Experimental: tensorflow datasets"
      ]
    },
    {
      "metadata": {
        "id": "4a7zPSbuOpmK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_data = tf.data.Dataset.from_tensor_slices({\"feature\": train_data, \"label\": train_labels})\n",
        "val_data = tf.data.Dataset.from_tensor_slices({\"feature\": val_data, \"label\": val_labels})\n",
        "train_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jXWGdlTBOpmN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_data.output_types"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gmsCTYwuOpmQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#build batches\n",
        "batch_size = 500\n",
        "train_data.shuffle(30000)\n",
        "batches = dataset.batch(batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O_zqsyoIOpmV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "sess = tf.Session()\n",
        "iterator = batches.make_one_shot_iterator()\n",
        "next_element = iterator.get_next()\n",
        "no_of_batches = int(np.ceil(labels.shape[0] / batch_size))\n",
        "counter = 1\n",
        "for i in tqdm(range(no_of_batches)):\n",
        "    value = sess.run(next_element)\n",
        "    print(value[\"feature\"].shape)\n",
        "    print(counter)\n",
        "    counter+=1\n",
        "\n",
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rG-K_AbzQmM3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# run this to check available gpu memory (i got 5gb)\n",
        "\n",
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}