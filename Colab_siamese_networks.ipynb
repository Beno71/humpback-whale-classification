{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab_siamese_networks.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Beno71/humpback-whale-classification/blob/master/Colab_siamese_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "caf9Pin0ddx6",
        "colab_type": "code",
        "outputId": "c2a2c495-4dc5-4125-9d4c-e2784456446f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "import os \n",
        "print(os.getcwd())\n",
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "drive  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KPOZ_GvIOpkR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# siamese networks for whale classification"
      ]
    },
    {
      "metadata": {
        "id": "miwpXH5NOpkX",
        "colab_type": "code",
        "outputId": "d03c5103-6a36-41a2-9bad-4589b3408d8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "cell_type": "code",
      "source": [
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.rendered_html { font-size: 18px; }</style>\"))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>.rendered_html { font-size: 18px; }</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "swbuAkt8Opkg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import os, time, itertools\n",
        "from skimage import io, transform\n",
        "import skimage\n",
        "import glob\n",
        "from tqdm import tnrange, tqdm\n",
        "from collections import Counter\n",
        "from random import shuffle\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "from IPython.display import clear_output\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bFYy7uuHOpkk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ]
    },
    {
      "metadata": {
        "id": "pLFTZG20Opkm",
        "colab_type": "code",
        "outputId": "9205bb27-3bfa-4f21-f313-a5efb8350218",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# some prep steps\n",
        "# for google colab\n",
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "  \n",
        "if IN_COLAB:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive/')\n",
        "  data_folder = \"/content/drive/My Drive/Colab Notebooks/data/\"\n",
        "else:\n",
        "  data_folder = \"data/\"\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "E4OYmCuGOpkq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#load .npz-file from folder\n",
        "\n",
        "\n",
        "\n",
        "loader = np.load(data_folder+\"humpback_300x100_gray_no_new.npz\")\n",
        "features = loader[\"features\"]\n",
        "labels = loader[\"labels\"]\n",
        "\n",
        "n_rows = labels.shape[0]\n",
        "\n",
        "\n",
        "\n",
        "split_ratio = 0.8\n",
        "data_size = 500\n",
        "expansion_factor = 8\n",
        "\n",
        "#take the sample from the most common classes\n",
        "ar = np.array(Counter(labels).most_common())\n",
        "count = 0\n",
        "label_list = []\n",
        "for idx, tup in enumerate(ar):\n",
        "    label_list.append(tup[0])\n",
        "    count += tup[1]\n",
        "    if count > data_size:\n",
        "        break\n",
        "label_list\n",
        "\n",
        "label_in_list=[x in label_list for x in labels]\n",
        "labels = labels[label_in_list]\n",
        "features = features[label_in_list]\n",
        "\n",
        "def standardize(X):\n",
        "    X = X.astype(np.float32)\n",
        "    X = (X - np.mean(X, axis=(1,2), keepdims=True)) / np.std(X, axis=(1,2), keepdims=True)\n",
        "    return X\n",
        "\n",
        "features = standardize(features)\n",
        "features = np.expand_dims(features, axis=1)\n",
        "\n",
        "all_combinations_without_labels = np.array(list(itertools.combinations(range(data_size),2)))\n",
        "same_label_indices = labels[all_combinations_without_labels[:,0]] == labels[all_combinations_without_labels[:,1]]\n",
        "all_combinations = np.append(all_combinations_without_labels, same_label_indices.reshape(-1,1), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dko54Bt-Opku",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_test_split_old(split_ratio):\n",
        "    \"\"\"\n",
        "    Get unique combinations from [0..data_size].\n",
        "    Shuffle same_label_pairs and dif_label_pairs individually.\n",
        "    Sample the first split_ratio*num pairs of each class for training and \n",
        "    sample the the rest (1-split_ratio)*num pairs of each class for validation\n",
        "    \n",
        "    \"\"\"\n",
        "    same_label_pairs = np.random.permutation(all_combinations[same_label_indices])\n",
        "    dif_label_pairs = np.random.permutation(all_combinations[~same_label_indices])\n",
        "    \n",
        "    num_pairs = int(1/2*expansion_factor*data_size)\n",
        "    num_train = int(split_ratio*num_pairs)\n",
        "   \n",
        "    train_matchings = np.append(same_label_pairs[:num_train], dif_label_pairs[:num_train], axis = 0)\n",
        "    val_matchings = np.append(same_label_pairs[num_train:num_pairs], dif_label_pairs[num_train:num_pairs], axis = 0)\n",
        "    \n",
        "    np.random.shuffle(train_matchings), np.random.shuffle(val_matchings)\n",
        "    \n",
        "    return train_matchings.astype(int), val_matchings.astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SdRbxqisOpk1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_test_split(split_ratio):\n",
        "    \"\"\"\n",
        "    First seperate all pictures into train_idx and val_idx. \n",
        "    Get unique combinations for each train_matchings and val_matchings.\n",
        "    Shuffle same_label_pairs and dif_label_pairs individually.\n",
        "    Sample the first split_ratio*num pairs of each class for training and \n",
        "    sample the the rest (1-split_ratio)*num pairs of each class for validation\n",
        "    \n",
        "    \"\"\"\n",
        "    perm = np.random.permutation(range(data_size))\n",
        "    train_idx = perm[:int(split_ratio*data_size)]\n",
        "    val_idx = perm[int(split_ratio*data_size):]\n",
        "    \n",
        "    def sample(idx, num_train):\n",
        "        combinations = np.array(list(itertools.combinations(idx,2)))\n",
        "\n",
        "        same_label_indices = labels[combinations[:,0]] == labels[combinations[:,1]]\n",
        "        combinations = np.append(combinations, same_label_indices.reshape(-1,1), axis=1)\n",
        "\n",
        "        same_label_pairs = np.random.permutation(combinations[same_label_indices])\n",
        "        dif_label_pairs = np.random.permutation(combinations[~same_label_indices])\n",
        "        \n",
        "        matchings = np.append(same_label_pairs[:num_train], dif_label_pairs[:num_train], axis = 0)\n",
        "        return matchings\n",
        "    \n",
        "    num_pairs = int(1/2*expansion_factor*data_size)\n",
        "    train_matchings = sample(train_idx, num_train=int(split_ratio*num_pairs))\n",
        "    val_matchings = sample(val_idx, num_train=int((1-split_ratio)*num_pairs))\n",
        "    \n",
        "    print(\"num train:\", train_matchings.shape, \"num val:\", val_matchings.shape)\n",
        "    \n",
        "    np.random.shuffle(train_matchings), np.random.shuffle(val_matchings)\n",
        "    return train_matchings.astype(int), val_matchings.astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V1sNvO_xOpk3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train Test Split"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "nMJQsDp1Opk4",
        "colab_type": "code",
        "outputId": "273b3d3c-7325-448c-97b3-d8e44c736b35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_matchings, val_matchings = train_test_split(split_ratio)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num train: (3200, 3) num val: (798, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EBOpothDsA_i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7ecb697c-aa98-492f-82e3-ebb1515bb391"
      },
      "cell_type": "code",
      "source": [
        "np.unique(labels).shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "3PCRBKf6Opk9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Helper functions for calculation of accuracy"
      ]
    },
    {
      "metadata": {
        "id": "tyXLeFapOpk9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_unique_N(iterable, N):\n",
        "    \"\"\"Yields (in order) the first N unique elements of iterable. \n",
        "    Might yield less if data too short.\"\"\"\n",
        "    seen = set()\n",
        "    for e in iterable:\n",
        "        if e in seen:\n",
        "            continue\n",
        "        seen.add(e)\n",
        "        yield e\n",
        "        if len(seen) == N:\n",
        "            return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xewGMyTWOplC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_k_nearest(distance_matrix):\n",
        "    dm = distance_matrix.copy()\n",
        "    for i in range(dm.shape[0]):    \n",
        "        dm[i,i] += 100000 \n",
        "    top5_nearest = np.empty((distance_matrix.shape[0], 5))\n",
        "    for idx, line in enumerate(dm):\n",
        "        sorted_indices = np.argsort(line)\n",
        "        top5_nearest[idx,:] = np.fromiter(get_unique_N(sorted_indices, 5), int)\n",
        "    top5_nearest = labels[top5_nearest.astype(int)]\n",
        "    return top5_nearest.astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Bg6gRoytoUDn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "weights_standard = np.array([1, 0.8, 0.6, 0.4, 0.2])\n",
        "weights_first = np.array([1,0,0,0,0])\n",
        "weights_half = np.array([1,0.5,0.33,0.25,0.20])\n",
        "def calculate_accuracy_score(outputs_1, outputs_2, weights=weights_standard):\n",
        "    distance_matrix = euclidean_distances(outputs_1, outputs_2)\n",
        "    top5_nearest = get_k_nearest(distance_matrix)\n",
        "    true_labels = np.repeat(np.array([labels]), 5, axis=0).T\n",
        "    prediction_matrix = top5_nearest == true_labels\n",
        "    scores_per_image = prediction_matrix@weights_standard\n",
        "    score = np.sum(scores_per_image)/scores_per_image.shape[0]\n",
        "    return score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TbUWhUeHOplG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def batch_data(num_data, batch_size):\n",
        "    \"\"\" Yield batches with indices until epoch is over.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    num_data: int\n",
        "        The number of samples in the dataset.\n",
        "    batch_size: int\n",
        "        The batch size used using training.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    batch_ixs: np.array of ints with shape [batch_size,]\n",
        "        Yields arrays of indices of size of the batch size until the epoch is over.\n",
        "    \"\"\"\n",
        "    \n",
        "    data_ixs = np.random.permutation(np.arange(num_data))\n",
        "    ix = 0\n",
        "    while ix + batch_size < num_data:\n",
        "        batch_ixs = data_ixs[ix:ix+batch_size]\n",
        "        ix += batch_size\n",
        "        yield batch_ixs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T1geGje-OplK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Check initializer of variables"
      ]
    },
    {
      "metadata": {
        "id": "ohoyQWFkOplL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SiamN:\n",
        "    \n",
        "    def __init__(self, name, learning_rate=0.001):\n",
        "        \n",
        "        self.name = name\n",
        "        self.dropout = tf.placeholder_with_default(0.0, shape=(), name=\"dropout\")\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weights =[]\n",
        "        self.biases =[]\n",
        "    \n",
        "    def build(self, length=300, height=100, channels=1, margin=0.5):\n",
        "\n",
        "        self.X_1 = tf.placeholder(shape=[None, channels, height, length], dtype=tf.float32, name=\"data_1\") #[NxD]\n",
        "        self.X_2 = tf.placeholder(shape=[None, channels, height, length], dtype=tf.float32, name=\"data_2\")\n",
        "        self.Y = tf.placeholder(shape=[None,], dtype=tf.float32, name=\"labels\") #[Nx1]\n",
        "        with tf.variable_scope(self.name, reuse=tf.AUTO_REUSE):\n",
        "        \n",
        "            first_hidden = self.X_1\n",
        "            first_conv1 = tf.layers.conv2d(inputs=first_hidden, filters=16, kernel_size=2, strides=1,\n",
        "                padding='same', activation = tf.nn.relu)\n",
        "            first_max_pool_1 = tf.layers.max_pooling2d(inputs=first_conv1, pool_size=2, strides=2, padding='same')\n",
        "            first_conv2 = tf.layers.conv2d(inputs=first_max_pool_1, filters=32, kernel_size=2, strides=1,\n",
        "                padding='same', activation = tf.nn.relu)\n",
        "            first_max_pool_2 = tf.layers.max_pooling2d(inputs=first_conv2, pool_size=5, strides=2, padding='same')\n",
        "            first_hidden = tf.contrib.layers.flatten(first_max_pool_2)\n",
        "            first_hidden = tf.nn.dropout(first_hidden, 1-self.dropout)\n",
        "            first_hidden = tf.contrib.layers.fully_connected(first_hidden, 200, activation_fn = None)\n",
        "            first_hidden = tf.contrib.layers.fully_connected(first_hidden, 20, activation_fn = None)\n",
        "\n",
        "            self.outputs_1 = first_hidden\n",
        "            \n",
        "            second_hidden = self.X_2\n",
        "            second_conv1 = tf.layers.conv2d(inputs=second_hidden, filters=16, kernel_size=2, strides=1,\n",
        "                padding='same', activation = tf.nn.relu)\n",
        "            second_max_pool_1 = tf.layers.max_pooling2d(inputs=second_conv1, pool_size=2, strides=2, padding='same')\n",
        "            second_conv2 = tf.layers.conv2d(inputs=second_max_pool_1, filters=32, kernel_size=2, strides=1,\n",
        "                padding='same', activation = tf.nn.relu)\n",
        "            second_max_pool_2 = tf.layers.max_pooling2d(inputs=second_conv2, pool_size=5, strides=2, padding='same')\n",
        "            second_hidden = tf.contrib.layers.flatten(second_max_pool_2)\n",
        "            second_hidden = tf.nn.dropout(second_hidden, 1-self.dropout)\n",
        "            second_hidden = tf.contrib.layers.fully_connected(second_hidden, 200, activation_fn = None)\n",
        "            second_hidden = tf.contrib.layers.fully_connected(second_hidden, 20, activation_fn = None)\n",
        "            \n",
        "            self.outputs_2 = second_hidden\n",
        "            \n",
        "            self.distance = tf.norm(first_hidden - second_hidden)\n",
        "            self.similarity = (1-self.Y) * tf.square(self.distance)                                           # keep the similar label (1) close to each other\n",
        "            self.dissimilarity = self.Y * tf.square(tf.maximum((margin - self.distance), 0))        # give penalty to dissimilar label if the distance is bigger than margin\n",
        "            self.loss = tf.reduce_mean((self.dissimilarity + self.similarity) / 2)\n",
        "            self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n",
        "            \n",
        "        \n",
        "    def train(self, features, train_matchings, val_matchings, epochs=20, dropout=0.0, batch_size=512):\n",
        "\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        scores = []\n",
        "        \n",
        "        config = tf.ConfigProto()\n",
        "        #config.gpu_options.allow_growth=True\n",
        "        self.session = tf.Session(config=config)\n",
        "        session = self.session\n",
        "        \n",
        "        session.run(tf.global_variables_initializer())\n",
        "        \n",
        "        tr_loss = session.run(self.loss, feed_dict={self.X_1: features[train_matchings[:,0]], self.X_2: features[train_matchings[:,1]], self.Y: train_matchings[:,2]})\n",
        "        val_loss = session.run(self.loss, feed_dict={self.X_1: features[val_matchings[:,0]], self.X_2: features[val_matchings[:,1]], self.Y: val_matchings[:,2]})\n",
        "        outputs_1 = session.run(self.outputs_1, feed_dict = {self.X_1:features})\n",
        "        outputs_2 = session.run(self.outputs_2, feed_dict = {self.X_2:features})\n",
        "        score = calculate_accuracy_score(outputs_1, outputs_2)\n",
        "        \n",
        "        train_losses.append(tr_loss)            \n",
        "        val_losses.append(val_loss)\n",
        "        scores.append(score)\n",
        "        print(f\"Epoch 0/{epochs} train_loss: {train_losses[-1]} val_loss: {val_losses[-1]} score: {scores[-1]}\")\n",
        "\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            if (epoch+1) % 5 == 0:\n",
        "                print(f\"Epoch {epoch+1}/{epochs} train_loss: {train_losses[-1]} val_loss: {val_losses[-1]} score: {scores[-1]}\")  \n",
        "            for batch_ixs in batch_data(train_matchings.shape[0], batch_size):\n",
        "                    _ = session.run( self.optimizer, feed_dict={self.X_1: features[train_matchings[batch_ixs,0]], self.X_2: features[train_matchings[batch_ixs,1]], self.Y: train_matchings[batch_ixs,2]})  \n",
        "            tr_loss = session.run(self.loss, feed_dict={self.X_1: features[train_matchings[:,0]], self.X_2: features[train_matchings[:,1]], self.Y: train_matchings[:,2]})\n",
        "            val_loss = session.run(self.loss, feed_dict={self.X_1: features[val_matchings[:,0]], self.X_2: features[val_matchings[:,1]], self.Y: val_matchings[:,2]})\n",
        "            train_losses.append(round(tr_loss/train_matchings.shape[0], 7))\n",
        "            val_losses.append(round(val_loss/val_matchings.shape[0], 7))\n",
        "            \n",
        "            outputs_1 = session.run(self.outputs_1, feed_dict = {self.X_1:features})\n",
        "            outputs_2 = session.run(self.outputs_2, feed_dict = {self.X_2:features})\n",
        "            score = calculate_accuracy_score(outputs_1, outputs_2)\n",
        "            scores.append(score)\n",
        "\n",
        "        \n",
        "        \n",
        "        self.hist={'train_loss': np.array(train_losses),\n",
        "           'val_loss': np.array(val_losses), \"epochs_trained\": epoch}\n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HUbxBg0DOplO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model1.session.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VGIBmg0cOplV",
        "colab_type": "code",
        "outputId": "58e95eb2-5ff6-4329-f06e-0e515cb06492",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        }
      },
      "cell_type": "code",
      "source": [
        "#You can change layer types and the number of neurons by changing the following variables.\n",
        "t = time.time()\n",
        "epochs = 150\n",
        "batch_size = 256\n",
        "\n",
        "\n",
        "model1 = SiamN(\"first_model\", learning_rate = 0.001)\n",
        "model1.build(margin=1)\n",
        "\n",
        "model1.train(features, train_matchings, val_matchings, epochs, batch_size=batch_size)\n",
        "print(\"Training finished in\", time.time()-t,\"s.\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/150 train_loss: 12894.1103515625 val_loss: 3243.61376953125 score: 0.3758620689655172\n",
            "Epoch 5/150 train_loss: 0.0791438 val_loss: 0.1157473 score: 0.37126436781609196\n",
            "Epoch 10/150 train_loss: 0.0306044 val_loss: 0.075247 score: 0.35977011494252875\n",
            "Epoch 15/150 train_loss: 0.0181373 val_loss: 0.061572 score: 0.37049808429118775\n",
            "Epoch 20/150 train_loss: 0.012261 val_loss: 0.0532961 score: 0.37394636015325666\n",
            "Epoch 25/150 train_loss: 0.0089356 val_loss: 0.0482863 score: 0.3731800766283525\n",
            "Epoch 30/150 train_loss: 0.0068351 val_loss: 0.0442578 score: 0.38199233716475095\n",
            "Epoch 35/150 train_loss: 0.0054003 val_loss: 0.0412756 score: 0.39233716475095787\n",
            "Epoch 40/150 train_loss: 0.004378 val_loss: 0.0388556 score: 0.41762452107279696\n",
            "Epoch 45/150 train_loss: 0.0036231 val_loss: 0.0368768 score: 0.4061302681992337\n",
            "Epoch 50/150 train_loss: 0.0030358 val_loss: 0.035345 score: 0.3996168582375479\n",
            "Epoch 55/150 train_loss: 0.0025766 val_loss: 0.0339642 score: 0.41226053639846744\n",
            "Epoch 60/150 train_loss: 0.0022187 val_loss: 0.0327012 score: 0.4318007662835249\n",
            "Epoch 65/150 train_loss: 0.0019225 val_loss: 0.031721 score: 0.44521072796934874\n",
            "Epoch 70/150 train_loss: 0.0016807 val_loss: 0.0309101 score: 0.4452107279693487\n",
            "Epoch 75/150 train_loss: 0.0014735 val_loss: 0.0301386 score: 0.4340996168582376\n",
            "Epoch 80/150 train_loss: 0.001309 val_loss: 0.0295002 score: 0.4256704980842912\n",
            "Epoch 85/150 train_loss: 0.0011633 val_loss: 0.0288223 score: 0.44980842911877394\n",
            "Epoch 90/150 train_loss: 0.0010397 val_loss: 0.0282425 score: 0.4482758620689655\n",
            "Epoch 95/150 train_loss: 0.0009363 val_loss: 0.0277962 score: 0.43524904214559385\n",
            "Epoch 100/150 train_loss: 0.0008446 val_loss: 0.0273601 score: 0.4528735632183908\n",
            "Epoch 105/150 train_loss: 0.0007689 val_loss: 0.0269007 score: 0.4555555555555556\n",
            "Epoch 110/150 train_loss: 0.0007066 val_loss: 0.0265566 score: 0.46283524904214557\n",
            "Epoch 115/150 train_loss: 0.0006529 val_loss: 0.0262356 score: 0.4628352490421456\n",
            "Epoch 120/150 train_loss: 0.0006075 val_loss: 0.025973 score: 0.478544061302682\n",
            "Epoch 125/150 train_loss: 0.0005681 val_loss: 0.0256923 score: 0.46973180076628357\n",
            "Epoch 130/150 train_loss: 0.0005331 val_loss: 0.0254912 score: 0.46283524904214557\n",
            "Epoch 135/150 train_loss: 0.0005018 val_loss: 0.0252769 score: 0.4544061302681992\n",
            "Epoch 140/150 train_loss: 0.0004743 val_loss: 0.0250979 score: 0.4846743295019157\n",
            "Epoch 145/150 train_loss: 0.0004517 val_loss: 0.0249013 score: 0.45708812260536397\n",
            "Epoch 150/150 train_loss: 0.0004316 val_loss: 0.0247743 score: 0.45938697318007665\n",
            "Training finished in 188.67906761169434 s.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eJDSOUmfzXTW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "scores_1, scores_2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LyOnBVXNOpla",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "calculate_accuracy_score(outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "97A_8CIdOpl5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XA0UdlPDOpl-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "idx = np.random.randint(3200)\n",
        "pic_a = features[train_matchings[idx,0]]\n",
        "pic_b = features[train_matchings[idx,1]]\n",
        "print(f\"same whales? {bool(train_matchings[idx,2])}\")    \n",
        "print(\"Index: \", idx)\n",
        "distance = model1.session.run(model1.distance, feed_dict={model1.X_1: np.array([pic_a]), model1.X_2: np.array([pic_b]), model1.Y: np.array([train_matchings[idx,2]])})\n",
        "print(f\"distance: {distance}\")\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.imshow(pic_a[0], cmap=\"gray\")\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.imshow(pic_b[0], cmap=\"gray\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c3hVYFY6OpmA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"model 1\")\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(model1.hist['train_loss'][5::], label=\"Training\")\n",
        "plt.plot(model1.hist['val_loss'][5::], label=\"Validation\")\n",
        "\n",
        "plt.xlabel(\"Epoch\", fontsize=20)\n",
        "plt.ylabel(\"Loss\", fontsize=20)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qafg5OErOpmE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(model1.hist[\"train_loss\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4Rb7k5OAOpmI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Experimental: tensorflow datasets"
      ]
    },
    {
      "metadata": {
        "id": "4a7zPSbuOpmK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_data = tf.data.Dataset.from_tensor_slices({\"feature\": train_data, \"label\": train_labels})\n",
        "val_data = tf.data.Dataset.from_tensor_slices({\"feature\": val_data, \"label\": val_labels})\n",
        "train_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jXWGdlTBOpmN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_data.output_types"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gmsCTYwuOpmQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#build batches\n",
        "batch_size = 500\n",
        "train_data.shuffle(30000)\n",
        "batches = dataset.batch(batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O_zqsyoIOpmV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "sess = tf.Session()\n",
        "iterator = batches.make_one_shot_iterator()\n",
        "next_element = iterator.get_next()\n",
        "no_of_batches = int(np.ceil(labels.shape[0] / batch_size))\n",
        "counter = 1\n",
        "for i in tqdm(range(no_of_batches)):\n",
        "    value = sess.run(next_element)\n",
        "    print(value[\"feature\"].shape)\n",
        "    print(counter)\n",
        "    counter+=1\n",
        "\n",
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}