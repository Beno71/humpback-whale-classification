{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab_siamese_networks.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Beno71/humpback-whale-classification/blob/master/Colab_siamese_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "caf9Pin0ddx6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os \n",
        "print(os.getcwd())\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KPOZ_GvIOpkR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# siamese networks for whale classification"
      ]
    },
    {
      "metadata": {
        "id": "miwpXH5NOpkX",
        "colab_type": "code",
        "outputId": "abdd2b6f-d5d8-456d-9ab0-f953dab6d360",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "cell_type": "code",
      "source": [
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.rendered_html { font-size: 18px; }</style>\"))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>.rendered_html { font-size: 18px; }</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "swbuAkt8Opkg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import os, time, itertools\n",
        "from skimage import io, transform\n",
        "import skimage\n",
        "import glob\n",
        "from tqdm import tnrange, tqdm\n",
        "from collections import Counter\n",
        "from random import shuffle\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "from IPython.display import clear_output\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bFYy7uuHOpkk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ]
    },
    {
      "metadata": {
        "id": "pLFTZG20Opkm",
        "colab_type": "code",
        "outputId": "9bad437f-7fcf-42be-892e-5008bf23a3e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# some prep steps\n",
        "# for google colab\n",
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "  \n",
        "if IN_COLAB:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive/')\n",
        "  data_folder = \"/content/drive/My Drive/Colab Notebooks/data/\"\n",
        "else:\n",
        "  data_folder = \"data/\"\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "E4OYmCuGOpkq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#load .npz-file from folder\n",
        "\n",
        "\n",
        "\n",
        "loader = np.load(data_folder+\"humpback_300x100_gray_no_new.npz\")\n",
        "features = loader[\"features\"]\n",
        "labels = loader[\"labels\"]\n",
        "\n",
        "n_rows = labels.shape[0]\n",
        "\n",
        "\n",
        "\n",
        "split_ratio = 0.8\n",
        "data_size = 400\n",
        "expansion_factor = 1\n",
        "\n",
        "#take the sample from the most common classes\n",
        "ar = np.array(Counter(labels).most_common())\n",
        "count = 0\n",
        "label_list = []\n",
        "for idx, tup in enumerate(ar):\n",
        "    label_list.append(tup[0])\n",
        "    count += tup[1]\n",
        "    if count > data_size:\n",
        "        break\n",
        "label_list\n",
        "\n",
        "label_in_list=[x in label_list for x in labels]\n",
        "labels = labels[label_in_list]\n",
        "features = features[label_in_list]\n",
        "\n",
        "def standardize(X):\n",
        "    X = X.astype(np.float32)\n",
        "    X = (X - np.mean(X, axis=(1,2), keepdims=True)) / np.std(X, axis=(1,2), keepdims=True)\n",
        "    return X\n",
        "\n",
        "features = standardize(features)\n",
        "features = np.expand_dims(features, axis=1)\n",
        "\n",
        "# not needed\n",
        "#all_combinations_without_labels = np.array(list(itertools.combinations(range(data_size),2)))\n",
        "#same_label_indices = labels[all_combinations_without_labels[:,0]] == labels[all_combinations_without_labels[:,1]]\n",
        "#all_combinations = np.append(all_combinations_without_labels, same_label_indices.reshape(-1,1), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x0nialoEJH3d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f62f26fc-a09c-48f5-e94a-d352fc65ace2"
      },
      "cell_type": "code",
      "source": [
        "np.unique(labels)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 12,  18,  32,  58, 166, 241, 265])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "metadata": {
        "id": "dko54Bt-Opku",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_test_split_old(split_ratio):\n",
        "    \"\"\"\n",
        "    Get unique combinations from [0..data_size].\n",
        "    Shuffle same_label_pairs and dif_label_pairs individually.\n",
        "    Sample the first split_ratio*num pairs of each class for training and \n",
        "    sample the the rest (1-split_ratio)*num pairs of each class for validation\n",
        "    \n",
        "    \"\"\"\n",
        "    same_label_pairs = np.random.permutation(all_combinations[same_label_indices])\n",
        "    dif_label_pairs = np.random.permutation(all_combinations[~same_label_indices])\n",
        "    \n",
        "    num_pairs = int(1/2*expansion_factor*data_size)\n",
        "    num_train = int(split_ratio*num_pairs)\n",
        "   \n",
        "    train_matchings = np.append(same_label_pairs[:num_train], dif_label_pairs[:num_train], axis = 0)\n",
        "    val_matchings = np.append(same_label_pairs[num_train:num_pairs], dif_label_pairs[num_train:num_pairs], axis = 0)\n",
        "    \n",
        "    np.random.shuffle(train_matchings), np.random.shuffle(val_matchings)\n",
        "    \n",
        "    return train_matchings.astype(int), val_matchings.astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SdRbxqisOpk1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_test_split(split_ratio):\n",
        "    \"\"\"\n",
        "    Split data into train, validation and test set.\n",
        "    Test set contains just pictures. \n",
        "    Validation and Train set are pairs of pictures (unique matches).\n",
        "    \"\"\"\n",
        "    perm = np.random.permutation(range(data_size))\n",
        "    train_idx = perm[:int(split_ratio*data_size)]\n",
        "    test_idx = perm[int(split_ratio*data_size):]\n",
        "    \n",
        "    combinations = np.array(list(itertools.combinations(train_idx,2)))\n",
        "    \n",
        "    same_label_indices = labels[combinations[:,0]] == labels[combinations[:,1]]\n",
        "    combinations = np.append(combinations, same_label_indices.reshape(-1,1), axis=1)\n",
        "\n",
        "    same_label_pairs = np.random.permutation(combinations[same_label_indices])\n",
        "    dif_label_pairs = np.random.permutation(combinations[~same_label_indices])\n",
        "    \n",
        "    num_pairs = int(1/2*expansion_factor*data_size)\n",
        "    num_train = int(split_ratio*num_pairs)\n",
        "   \n",
        "    train_matchings = np.append(same_label_pairs[:num_train], dif_label_pairs[:num_train], axis = 0)\n",
        "    val_matchings = np.append(same_label_pairs[num_train:num_pairs], dif_label_pairs[num_train:num_pairs], axis = 0)\n",
        "    \n",
        "    np.random.shuffle(train_matchings), np.random.shuffle(val_matchings)\n",
        "    return train_matchings.astype(int), val_matchings.astype(int), test_idx.astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V1sNvO_xOpk3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train Test Split"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "nMJQsDp1Opk4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cb7c5004-8e5a-4eb4-b8d4-60c2730eb6a6"
      },
      "cell_type": "code",
      "source": [
        "train_matchings, val_matchings, test_idx = train_test_split(split_ratio)\n",
        "train_matchings.shape, val_matchings.shape, test_idx.shape\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((320, 3), (80, 3), (80,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "metadata": {
        "id": "3PCRBKf6Opk9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Helper functions for calculation of accuracy"
      ]
    },
    {
      "metadata": {
        "id": "Un1GL39342oE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cca2db0a-d98a-450f-dc56-fcb8c51d1f72"
      },
      "cell_type": "code",
      "source": [
        "a = np.array([1,1,2,1,4,5,6,7])\n",
        "np.argsort(a)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 3, 2, 4, 5, 6, 7])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "metadata": {
        "id": "tyXLeFapOpk9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_unique_N(iterable, N):\n",
        "    \"\"\"Yields (in order) the first N unique elements of iterable. \n",
        "    Might yield less if data too short.\"\"\"\n",
        "    seen = set()\n",
        "    for e in iterable:\n",
        "        if e in seen: # ES IST NIE IN SEEN :D\n",
        "            continue\n",
        "        seen.add(e)\n",
        "        yield e\n",
        "        if len(seen) == N:\n",
        "            return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xewGMyTWOplC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_k_nearest(distance_matrix):\n",
        "    dm = distance_matrix.copy()\n",
        "    for i in range(dm.shape[1]):    \n",
        "        dm[i,i] += 100000 \n",
        "    top5_nearest = np.empty((distance_matrix.shape[0], 5))\n",
        "    for idx, line in enumerate(dm):\n",
        "        sorted_indices = np.argsort(line)\n",
        "        top5_nearest[idx,:] = np.fromiter(get_unique_N(labels[sorted_indices], 5), int)\n",
        "    #top5_nearest = labels[top5_nearest.astype(int)]\n",
        "    return top5_nearest.astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Bg6gRoytoUDn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "weights_standard = np.array([1, 0.8, 0.6, 0.4, 0.2])\n",
        "weights_first = np.array([1,0,0,0,0])\n",
        "weights_half = np.array([1,0.5,0.33,0.25,0.20])\n",
        "def calculate_accuracy_score(outputs_1, outputs_2=None, weights=weights_standard):\n",
        "    distance_matrix = euclidean_distances(outputs_1, outputs_1[test_idx])\n",
        "    top5_nearest = get_k_nearest(distance_matrix)\n",
        "    #print(top5_nearest)\n",
        "    true_labels = np.repeat(np.array([labels]), 5, axis=0).T\n",
        "    prediction_matrix = top5_nearest == true_labels\n",
        "    #print(np.count_nonzero(prediction_matrix))\n",
        "    for row in prediction_matrix:\n",
        "        for i, elt in enumerate(row):\n",
        "            if elt:\n",
        "                row[i+1:] = 0\n",
        "                break\n",
        "    #print(np.count_nonzero(prediction_matrix),\"/\",labels.shape[0],\"unique whales have correct labels in the top 5\")\n",
        "    scores_per_image = prediction_matrix@weights_half\n",
        "    score = np.sum(scores_per_image)/scores_per_image.shape[0]\n",
        "    return score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TbUWhUeHOplG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def batch_data(num_data, batch_size):\n",
        "    \"\"\" Yield batches with indices until epoch is over.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    num_data: int\n",
        "        The number of samples in the dataset.\n",
        "    batch_size: int\n",
        "        The batch size used using training.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    batch_ixs: np.array of ints with shape [batch_size,]\n",
        "        Yields arrays of indices of size of the batch size until the epoch is over.\n",
        "    \"\"\"\n",
        "    \n",
        "    data_ixs = np.random.permutation(np.arange(num_data))\n",
        "    ix = 0\n",
        "    while ix + batch_size < num_data:\n",
        "        batch_ixs = data_ixs[ix:ix+batch_size]\n",
        "        ix += batch_size\n",
        "        yield batch_ixs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MGLEBx2s4GsV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1b28029d-cafb-419f-a14e-1ef1e4136229"
      },
      "cell_type": "code",
      "source": [
        "np.unique(labels)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 12,  18,  32,  58, 166, 241, 265, 416, 502])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "T1geGje-OplK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Check initializer of variables"
      ]
    },
    {
      "metadata": {
        "id": "ohoyQWFkOplL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SiamN:\n",
        "    \n",
        "    def __init__(self, name, learning_rate=0.001, length=300, height=100, channels=1, margin=0.5):\n",
        "        \n",
        "        self.name = name\n",
        "        self.dropout = tf.placeholder_with_default(0.0, shape=(), name=\"dropout\")\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weights =[]\n",
        "        self.biases =[]\n",
        "        self.margin = margin\n",
        "        \n",
        "        self.X1 = tf.placeholder(shape=[None, channels, height, length], dtype=tf.float32, name=\"data_1\") \n",
        "        self.X2 = tf.placeholder(shape=[None, channels, height, length], dtype=tf.float32, name=\"data_2\") \n",
        "        self.Y = tf.placeholder(shape=[None,], dtype=tf.float32, name=\"labels\") \n",
        "        \n",
        "        self.output1 = self.forward_pass(self.X1, reuse=False)\n",
        "        self.output2 = self.forward_pass(self.X2, reuse=True)\n",
        "        self.loss = tf.contrib.losses.metric_learning.contrastive_loss(self.Y, self.output1, self.output2, self.margin)\n",
        "        #self.loss = self.contrastive_loss(self.Y, self.output1, self.output2, self.margin)\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n",
        "        \n",
        "    \n",
        "    def forward_pass(self, X, reuse=False):\n",
        "        \n",
        "        convkwargs = {'padding': 'same', 'activation_fn': tf.nn.relu, 'reuse': reuse}\n",
        "        poolkwargs = {'kernel_size': 2, 'stride': 2, 'padding': 'same'}\n",
        "        \n",
        "        with tf.variable_scope(\"conv1\") as scope:\n",
        "            x = tf.contrib.layers.conv2d(X, 64, kernel_size=9, stride=2, scope=scope, **convkwargs)\n",
        "            x = tf.contrib.layers.batch_norm(x, scope=scope, reuse=reuse)\n",
        "            x = tf.contrib.layers.max_pool2d(x, **poolkwargs)\n",
        "        with tf.variable_scope(\"conv2\") as scope:\n",
        "            x = tf.contrib.layers.conv2d(x, 64, kernel_size=7, stride=1, scope=scope, **convkwargs)\n",
        "            x = tf.contrib.layers.batch_norm(x, scope=scope, reuse=reuse)\n",
        "            x = tf.contrib.layers.max_pool2d(x, **poolkwargs)\n",
        "        with tf.variable_scope(\"conv3\") as scope:\n",
        "            x = tf.contrib.layers.conv2d(x, 128, kernel_size=5, stride=1, scope=scope, **convkwargs)\n",
        "            x = tf.contrib.layers.batch_norm(x, scope=scope, reuse=reuse)\n",
        "            x = tf.contrib.layers.max_pool2d(x, **poolkwargs)\n",
        "        with tf.variable_scope(\"conv4\") as scope:\n",
        "            x = tf.contrib.layers.conv2d(x, 256, kernel_size=3, stride=1, scope=scope, **convkwargs)\n",
        "            x = tf.contrib.layers.batch_norm(x, scope=scope, reuse=reuse)\n",
        "            x = tf.contrib.layers.max_pool2d(x, **poolkwargs)\n",
        "                \n",
        "        \n",
        "        x = tf.contrib.layers.flatten(x)\n",
        "        x = tf.contrib.layers.fully_connected(x, 500)\n",
        "        x = tf.contrib.layers.fully_connected(x, 20)\n",
        "        \n",
        "        output = x\n",
        "            \n",
        "        return output\n",
        "            \n",
        "        #deprecated\n",
        "    def contrastive_loss(self, Y, output1, output2, margin=0.5):\n",
        "        distance = tf.norm(output2 - output1)\n",
        "        similarity = Y * tf.square(distance)                                           # keep the similar label (1) close to each other\n",
        "        dissimilarity = (1-Y) * tf.square(tf.maximum((margin - distance), 0))        # give penalty to dissimilar label if the distance is bigger than margin\n",
        "        loss = tf.reduce_mean((dissimilarity + similarity) / 2)\n",
        "        return loss\n",
        "\n",
        "    \n",
        "    def train(self, features, train_matchings, val_matchings, epochs=20, dropout=0.0, batch_size=512):\n",
        "\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        acc_scores = []\n",
        "        \n",
        "        config = tf.ConfigProto()\n",
        "        #config.gpu_options.allow_growth=True\n",
        "        self.session = tf.Session(config=config)\n",
        "        session = self.session\n",
        "        \n",
        "        session.run(tf.global_variables_initializer())\n",
        "        \n",
        "        train_loss = session.run(self.loss, feed_dict={self.X1: features[train_matchings[:,0]], self.X2: features[train_matchings[:,1]], self.Y: train_matchings[:,2]})\n",
        "        val_loss = session.run(self.loss, feed_dict={self.X1: features[val_matchings[:,0]], self.X2: features[val_matchings[:,1]], self.Y: val_matchings[:,2]})\n",
        "        \n",
        "        acc_score_output = session.run(self.output1, feed_dict={self.X1: features})\n",
        "        acc_score = calculate_accuracy_score(acc_score_output)\n",
        "                \n",
        "        train_losses.append(round(train_loss/train_matchings.shape[0], 7))\n",
        "        val_losses.append(round(val_loss/val_matchings.shape[0], 7))\n",
        "        acc_scores.append(int(acc_score*100))\n",
        "        print(f\"Epoch 0/{epochs} train_loss: {train_losses[-1]} val_loss: {val_losses[-1]} acc_score: {acc_scores[-1]}\")\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            if (epoch+1) % 5 == 0:\n",
        "                print(f\"Epoch {epoch+1}/{epochs} train_loss: {train_losses[-1]} val_loss: {val_losses[-1]} acc_score: {acc_scores[-1]}\")  \n",
        "            for batch_ixs in batch_data(train_matchings.shape[0], batch_size):\n",
        "                    _ = session.run( self.optimizer, feed_dict={self.X1: features[train_matchings[batch_ixs,0]], self.X2: features[train_matchings[batch_ixs,1]], self.Y: train_matchings[batch_ixs,2]})  \n",
        "            \n",
        "            #TODO: remove boilerplate code, define function to calc accs and errors with flag print=TRUE/FALSE\n",
        "            train_loss = session.run(self.loss, feed_dict={self.X1: features[train_matchings[:,0]], self.X2: features[train_matchings[:,1]], self.Y: train_matchings[:,2]})\n",
        "            val_loss = session.run(self.loss, feed_dict={self.X1: features[val_matchings[:,0]], self.X2: features[val_matchings[:,1]], self.Y: val_matchings[:,2]})\n",
        "\n",
        "            output = session.run(self.output1, feed_dict={self.X1: features})\n",
        "            acc_score = calculate_accuracy_score(output)\n",
        "\n",
        "            #train_losses.append(round(train_loss/train_matchings.shape[0], 7))\n",
        "            #val_losses.append(round(val_loss/val_matchings.shape[0], 7))\n",
        "            train_losses.append(train_loss)\n",
        "            val_losses.append(val_loss)\n",
        "            acc_scores.append(int(acc_score*100))\n",
        "\n",
        "        \n",
        "        \n",
        "        self.hist={'train_loss': np.array(train_losses),\n",
        "           'val_loss': np.array(val_losses), \"epochs_trained\": epoch}\n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yv3NuyHD-8e-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "3b917dee-1c92-491e-897b-9b43006f3eca"
      },
      "cell_type": "code",
      "source": [
        "i#model1.session.close()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-e13dd0b59725>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mi\u001b[0m\u001b[0;31m#model1.session.close()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'i' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "VGIBmg0cOplV",
        "colab_type": "code",
        "outputId": "ca2c57eb-78c8-4f4b-84aa-d5e7b5e8178c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1408
        }
      },
      "cell_type": "code",
      "source": [
        "#You can change layer types and the number of neurons by changing the following variables.\n",
        "t = time.time()\n",
        "epochs = 200\n",
        "batch_size = 256\n",
        "\n",
        "tf.reset_default_graph()\n",
        "model1 = SiamN(\"first_model\", learning_rate = 0.0005, margin = 2)\n",
        "\n",
        "model1.train(features, train_matchings, val_matchings, epochs, batch_size=batch_size)\n",
        "print(\"Training finished in\", time.time()-t,\"s.\")\n",
        "\n",
        "\n",
        "#shape of tuple can not be built\n",
        "# in particular:\n",
        "# features[train_matchings[:,0]].shape.ndims\n",
        "# probably should be tensor instead of np.array"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/200 train_loss: 0.0517379 val_loss: 0.2197217 acc_score: 34\n",
            "Epoch 5/200 train_loss: 0.8946079015731812 val_loss: 1.4131824970245361 acc_score: 35\n",
            "Epoch 10/200 train_loss: 0.37902483344078064 val_loss: 1.3338706493377686 acc_score: 36\n",
            "Epoch 15/200 train_loss: 0.2908611297607422 val_loss: 1.3860896825790405 acc_score: 37\n",
            "Epoch 20/200 train_loss: 0.25481367111206055 val_loss: 1.679944634437561 acc_score: 34\n",
            "Epoch 25/200 train_loss: 0.2160298377275467 val_loss: 1.6481382846832275 acc_score: 35\n",
            "Epoch 30/200 train_loss: 0.20402464270591736 val_loss: 1.7248280048370361 acc_score: 37\n",
            "Epoch 35/200 train_loss: 0.20690540969371796 val_loss: 1.861559510231018 acc_score: 34\n",
            "Epoch 40/200 train_loss: 0.21182648837566376 val_loss: 2.0203349590301514 acc_score: 34\n",
            "Epoch 45/200 train_loss: 0.20793752372264862 val_loss: 1.94972825050354 acc_score: 34\n",
            "Epoch 50/200 train_loss: 0.20287743210792542 val_loss: 2.1302225589752197 acc_score: 32\n",
            "Epoch 55/200 train_loss: 0.19625599682331085 val_loss: 1.9651758670806885 acc_score: 32\n",
            "Epoch 60/200 train_loss: 0.19747008383274078 val_loss: 1.9176117181777954 acc_score: 34\n",
            "Epoch 65/200 train_loss: 0.20436151325702667 val_loss: 2.0537493228912354 acc_score: 34\n",
            "Epoch 70/200 train_loss: 0.2034798562526703 val_loss: 2.077500581741333 acc_score: 32\n",
            "Epoch 75/200 train_loss: 0.20850157737731934 val_loss: 1.9791139364242554 acc_score: 33\n",
            "Epoch 80/200 train_loss: 0.21147212386131287 val_loss: 2.0024962425231934 acc_score: 34\n",
            "Epoch 85/200 train_loss: 0.21810758113861084 val_loss: 2.1260623931884766 acc_score: 34\n",
            "Epoch 90/200 train_loss: 0.20535418391227722 val_loss: 2.0619146823883057 acc_score: 34\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-16d4c464c60f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSiamN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"first_model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmargin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_matchings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_matchings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training finished in\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"s.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-50-7df7acac51a8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, features, train_matchings, val_matchings, epochs, dropout, batch_size)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_matchings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_matchings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval_matchings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0macc_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_accuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "MQ8Lv095xC3r",
        "colab_type": "code",
        "outputId": "c2c28236-d64c-4109-cc37-c74014930d73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "test = tf.constant([[1,2,3], [1,2,3]])\n",
        "test.shape.ndims"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "metadata": {
        "id": "8yXNZD__vwdA",
        "colab_type": "code",
        "outputId": "7afb5d85-07cc-4e41-f8dc-0f9fbce1d991",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "import pdb; pdb.pm()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py(861)_get_single_variable()\n",
            "-> name, \"\".join(traceback.format_list(tb))))\n",
            "(Pdb) model1.X1\n",
            "*** NameError: name 'model1' is not defined\n",
            "(Pdb) quit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eJDSOUmfzXTW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "scores_1, scores_2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LyOnBVXNOpla",
        "colab_type": "code",
        "outputId": "b3ee0e98-d4d4-451c-cc1c-86c5b85e89f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "cell_type": "code",
      "source": [
        "calculate_accuracy_score(outputs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-122-2cdb18d21b11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcalculate_accuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'outputs' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "97A_8CIdOpl5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XA0UdlPDOpl-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "idx = np.random.randint(3200)\n",
        "pic_a = features[train_matchings[idx,0]]\n",
        "pic_b = features[train_matchings[idx,1]]\n",
        "print(f\"same whales? {bool(train_matchings[idx,2])}\")    \n",
        "print(\"Index: \", idx)\n",
        "distance = model1.session.run(model1.distance, feed_dict={model1.X_1: np.array([pic_a]), model1.X_2: np.array([pic_b]), model1.Y: np.array([train_matchings[idx,2]])})\n",
        "print(f\"distance: {distance}\")\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.imshow(pic_a[0], cmap=\"gray\")\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.imshow(pic_b[0], cmap=\"gray\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c3hVYFY6OpmA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"model 1\")\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(model1.hist['train_loss'][5::], label=\"Training\")\n",
        "plt.plot(model1.hist['val_loss'][5::], label=\"Validation\")\n",
        "\n",
        "plt.xlabel(\"Epoch\", fontsize=20)\n",
        "plt.ylabel(\"Loss\", fontsize=20)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qafg5OErOpmE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(model1.hist[\"train_loss\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4Rb7k5OAOpmI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Experimental: tensorflow datasets"
      ]
    },
    {
      "metadata": {
        "id": "4a7zPSbuOpmK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_data = tf.data.Dataset.from_tensor_slices({\"feature\": train_data, \"label\": train_labels})\n",
        "val_data = tf.data.Dataset.from_tensor_slices({\"feature\": val_data, \"label\": val_labels})\n",
        "train_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jXWGdlTBOpmN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_data.output_types"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gmsCTYwuOpmQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#build batches\n",
        "batch_size = 500\n",
        "train_data.shuffle(30000)\n",
        "batches = dataset.batch(batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O_zqsyoIOpmV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "sess = tf.Session()\n",
        "iterator = batches.make_one_shot_iterator()\n",
        "next_element = iterator.get_next()\n",
        "no_of_batches = int(np.ceil(labels.shape[0] / batch_size))\n",
        "counter = 1\n",
        "for i in tqdm(range(no_of_batches)):\n",
        "    value = sess.run(next_element)\n",
        "    print(value[\"feature\"].shape)\n",
        "    print(counter)\n",
        "    counter+=1\n",
        "\n",
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rG-K_AbzQmM3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# run this to check available gpu memory (i got 5gb)\n",
        "\n",
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}