{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# siamese networks for whale classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.rendered_html { font-size: 18px; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.rendered_html { font-size: 18px; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "from skimage import io, transform\n",
    "import skimage\n",
    "import glob\n",
    "from tqdm import tnrange, tqdm\n",
    "from collections import Counter\n",
    "from random import shuffle\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some prep steps\n",
    "data_folder = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load .npz-file from folder\n",
    "\n",
    "\n",
    "\n",
    "loader = np.load(data_folder+\"humpback_300x100_gray_no_new.npz\")\n",
    "features = loader[\"features\"]\n",
    "labels = loader[\"labels\"]\n",
    "\n",
    "n_rows = labels.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "train_ratio = 0.8\n",
    "data_size = 500\n",
    "expansion_factor = 8\n",
    "\n",
    "#take the sample from the most common classes\n",
    "ar = np.array(Counter(labels).most_common())\n",
    "count = 0\n",
    "label_list = []\n",
    "for idx, tup in enumerate(ar):\n",
    "    label_list.append(tup[0])\n",
    "    count += tup[1]\n",
    "    if count > data_size:\n",
    "        break\n",
    "label_list\n",
    "\n",
    "label_in_list=[x in label_list for x in labels]\n",
    "labels = labels[label_in_list]\n",
    "features = features[label_in_list]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#TODO: import standardize function\n",
    "\n",
    "def standardize(X):\n",
    "    X = X.astype(np.float32)\n",
    "    X = (X - np.mean(X, axis=(1,2), keepdims=True)) / np.std(X, axis=(1,2), keepdims=True)\n",
    "\n",
    "    return X\n",
    "\n",
    "features = standardize(features)\n",
    "features = np.expand_dims(features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(labels).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for calculation of accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_N(iterable, N):\n",
    "    \"\"\"Yields (in order) the first N unique elements of iterable. \n",
    "    Might yield less if data too short.\"\"\"\n",
    "    seen = set()\n",
    "    for e in iterable:\n",
    "        if e in seen:\n",
    "            continue\n",
    "        seen.add(e)\n",
    "        yield e\n",
    "        if len(seen) == N:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_nearest(distance_matrix):\n",
    "    dm = distance_matrix.copy()\n",
    "    for i in range(dm.shape[0]):    \n",
    "        dm[i,i] += 100000 \n",
    "    nearest = np.empty(shape=(dm.shape[0], dm.shape[0]))\n",
    "    for i in range(dm.shape[0]):\n",
    "        nearest[:,i] = np.argmin(dm, axis=0)\n",
    "        dm[nearest[:,i].astype(int)] += 100000\n",
    "        nearest[:,i] = labels[nearest[:,i].astype(int)]\n",
    "    top5_nearest = np.empty((distance_matrix.shape[0], 5))\n",
    "    for idx, line in enumerate(nearest):\n",
    "        #import pdb; pdb.set_trace()\n",
    "        top5_nearest[idx,:] = np.fromiter(get_unique_N(line, 5), int)\n",
    "    return top5_nearest.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutations = np.array(np.meshgrid(range(labels.shape[0]), range(labels.shape[0]))).T.reshape(-1,2)\n",
    "matching_table = np.empty(shape=(labels.shape[0]**2, 3))\n",
    "matching_table[:,0:2] = permutations\n",
    "matching_table[:,2] = labels[permutations[:,0]]==labels[permutations[:,1]]\n",
    "\n",
    "same_label_indices = matching_table[:,2] == 1\n",
    "dif_label_indices = ~same_label_indices\n",
    "\n",
    "np.random.seed(1223)\n",
    "same_label_pairs = np.random.permutation(matching_table[same_label_indices])[:int(1/2*expansion_factor*data_size)]\n",
    "dif_label_pairs = np.random.permutation(matching_table[dif_label_indices])[:int(1/2*expansion_factor*data_size)]\n",
    "matching_table = np.concatenate((same_label_pairs, dif_label_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4000, 3), (2000, 3), (2000, 3))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matching_table.shape, same_label_pairs.shape, dif_label_pairs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(522, 1, 100, 300)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(matching_table, split_ratio):\n",
    "    #import pdb; pdb.set_trace()\n",
    "    shuffled = np.random.permutation(matching_table)\n",
    "    train_matchings = shuffled[:int(split_ratio*expansion_factor*labels.shape[0])]\n",
    "    val_matchings = shuffled[int(split_ratio*expansion_factor*labels.shape[0]):]\n",
    "    return train_matchings.astype(int), val_matchings.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matchings, val_matchings = train_test_split(matching_table, train_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(num_data, batch_size):\n",
    "    \"\"\" Yield batches with indices until epoch is over.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_data: int\n",
    "        The number of samples in the dataset.\n",
    "    batch_size: int\n",
    "        The batch size used using training.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    batch_ixs: np.array of ints with shape [batch_size,]\n",
    "        Yields arrays of indices of size of the batch size until the epoch is over.\n",
    "    \"\"\"\n",
    "    \n",
    "    data_ixs = np.random.permutation(np.arange(num_data))\n",
    "    ix = 0\n",
    "    while ix + batch_size < num_data:\n",
    "        batch_ixs = data_ixs[ix:ix+batch_size]\n",
    "        ix += batch_size\n",
    "        yield batch_ixs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check initializer of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiamN:\n",
    "    \n",
    "    def __init__(self, name, learning_rate=0.001):\n",
    "        \n",
    "        self.name = name\n",
    "        self.dropout = tf.placeholder_with_default(0.0, shape=(), name=\"dropout\")\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights =[]\n",
    "        self.biases =[]\n",
    "    \n",
    "    def build(self, length=300, height=100, channels=1, margin=0.5):\n",
    "\n",
    "        self.X_1 = tf.placeholder(shape=[None, channels, height, length], dtype=tf.float32, name=\"data_1\") #[NxD]\n",
    "        self.X_2 = tf.placeholder(shape=[None, channels, height, length], dtype=tf.float32, name=\"data_2\")\n",
    "        self.Y = tf.placeholder(shape=[None,], dtype=tf.float32, name=\"labels\") #[Nx1]\n",
    "        with tf.variable_scope(self.name, reuse=tf.AUTO_REUSE):\n",
    "        \n",
    "            first_hidden = self.X_1\n",
    "            first_conv1 = tf.layers.conv2d(inputs=first_hidden, filters=16, kernel_size=2, strides=1,\n",
    "                padding='same', activation = tf.nn.relu)\n",
    "            first_max_pool_1 = tf.layers.max_pooling2d(inputs=first_conv1, pool_size=2, strides=2, padding='same')\n",
    "            first_conv2 = tf.layers.conv2d(inputs=first_max_pool_1, filters=32, kernel_size=2, strides=1,\n",
    "                padding='same', activation = tf.nn.relu)\n",
    "            first_max_pool_2 = tf.layers.max_pooling2d(inputs=first_conv2, pool_size=5, strides=2, padding='same')\n",
    "            first_hidden = tf.contrib.layers.flatten(first_max_pool_2)\n",
    "            first_hidden = tf.nn.dropout(first_hidden, 1-self.dropout)\n",
    "            first_hidden = tf.contrib.layers.fully_connected(first_hidden, 20, activation_fn = None)\n",
    "\n",
    "            self.outputs = first_hidden\n",
    "            \n",
    "            second_hidden = self.X_2\n",
    "            second_conv1 = tf.layers.conv2d(inputs=second_hidden, filters=16, kernel_size=2, strides=1,\n",
    "                padding='same', activation = tf.nn.relu)\n",
    "            second_max_pool_1 = tf.layers.max_pooling2d(inputs=second_conv1, pool_size=2, strides=2, padding='same')\n",
    "            second_conv2 = tf.layers.conv2d(inputs=second_max_pool_1, filters=32, kernel_size=2, strides=1,\n",
    "                padding='same', activation = tf.nn.relu)\n",
    "            second_max_pool_2 = tf.layers.max_pooling2d(inputs=second_conv2, pool_size=5, strides=2, padding='same')\n",
    "            second_hidden = tf.contrib.layers.flatten(second_max_pool_2)\n",
    "            second_hidden = tf.nn.dropout(second_hidden, 1-self.dropout)\n",
    "            second_hidden = tf.contrib.layers.fully_connected(second_hidden, 20, activation_fn = None)\n",
    "            \n",
    "            \n",
    "            self.distance = tf.norm(first_hidden - second_hidden)\n",
    "            self.similarity = (1-self.Y) * tf.square(self.distance)                                           # keep the similar label (1) close to each other\n",
    "            self.dissimilarity = self.Y * tf.square(tf.maximum((margin - self.distance), 0))        # give penalty to dissimilar label if the distance is bigger than margin\n",
    "            self.loss = tf.reduce_mean((self.dissimilarity + self.similarity) / 2)\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n",
    "            \n",
    "        \n",
    "    def train(self, features, train_matchings, val_matchings, epochs=20, dropout=0.0, batch_size=512):\n",
    "\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        \n",
    "        config = tf.ConfigProto()\n",
    "        #config.gpu_options.allow_growth=True\n",
    "        self.session = tf.Session(config=config)\n",
    "        session = self.session\n",
    "        \n",
    "        session.run(tf.global_variables_initializer())\n",
    "\n",
    "        tr_loss = session.run(self.loss, feed_dict={self.X_1: features[train_matchings[:,0]], self.X_2: features[train_matchings[:,1]], self.Y: train_matchings[:,2]})\n",
    "        val_loss = session.run(self.loss, feed_dict={self.X_1: features[val_matchings[:,0]], self.X_2: features[val_matchings[:,1]], self.Y: val_matchings[:,2]})\n",
    "        \n",
    "        train_losses.append(tr_loss)            \n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"Epoch 0/{epochs} train_loss: {train_losses[-1]} val_loss: {val_losses[-1]}\")\n",
    "\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            if (epoch+1) % 5 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs} train_loss: {train_losses[-1]} val_loss: {val_losses[-1]}\")  \n",
    "            for batch_ixs in batch_data(train_matchings.shape[0], batch_size):\n",
    "                    _ = session.run( self.optimizer, feed_dict={self.X_1: features[train_matchings[batch_ixs,0]], self.X_2: features[train_matchings[batch_ixs,1]], self.Y: train_matchings[batch_ixs,2]})  \n",
    "            tr_loss = session.run(self.loss, feed_dict={self.X_1: features[train_matchings[:,0]], self.X_2: features[train_matchings[:,1]], self.Y: train_matchings[:,2]})\n",
    "            val_loss = session.run(self.loss, feed_dict={self.X_1: features[val_matchings[:,0]], self.X_2: features[val_matchings[:,1]], self.Y: val_matchings[:,2]})\n",
    "            train_losses.append(round(tr_loss/train_matchings.shape[0], 7))\n",
    "            val_losses.append(round(val_loss/val_matchings.shape[0], 7))\n",
    "\n",
    "        \n",
    "        \n",
    "        self.hist={'train_loss': np.array(train_losses),\n",
    "           'val_loss': np.array(val_losses), \"epochs_trained\": epoch}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/50 train_loss: 11696.4423828125 val_loss: 2340.988525390625\n",
      "Epoch 5/50 train_loss: 0.0620109 val_loss: 0.0654656\n",
      "Epoch 10/50 train_loss: 0.0253317 val_loss: 0.0284388\n",
      "Epoch 15/50 train_loss: 0.0142666 val_loss: 0.01675\n",
      "Epoch 20/50 train_loss: 0.009286 val_loss: 0.0111948\n",
      "Epoch 25/50 train_loss: 0.0065309 val_loss: 0.0081024\n",
      "Epoch 30/50 train_loss: 0.0048402 val_loss: 0.0061502\n",
      "Epoch 35/50 train_loss: 0.0037232 val_loss: 0.0048494\n",
      "Epoch 40/50 train_loss: 0.0029656 val_loss: 0.0039297\n",
      "Epoch 45/50 train_loss: 0.0024033 val_loss: 0.0032296\n",
      "Epoch 50/50 train_loss: 0.0019951 val_loss: 0.0027131\n"
     ]
    }
   ],
   "source": [
    "#You can change layer types and the number of neurons by changing the following variables.\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 256\n",
    "\n",
    "\n",
    "model1 = SiamN(\"first_model\", learning_rate = 0.001)\n",
    "model1.build(margin=1)\n",
    "\n",
    "model1.train(features, train_matchings, val_matchings, epochs,\n",
    "                          batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model1.session.run(model1.outputs, feed_dict = {model1.X_1:features})\n",
    "distance_matrix = euclidean_distances(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(522,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pdb; pdb.pm()\n",
    "top5_nearest = get_k_nearest(distance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([0,0,0,1,1])\n",
    "np.argmin(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = np.repeat(np.array([labels]), 5, axis=0).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_matrix = top5_nearest == true_labels\n",
    "weights_standard = np.array([1, 0.8, 0.6, 0.4, 0.2])\n",
    "weights_first = np.array([1,0,0,0,0])\n",
    "weights_half = np.array([1,0.5,0.33,0.25,0.20])\n",
    "scores_per_image = prediction_matrix@weights_standard\n",
    "score = np.sum(scores_per_image)/scores_per_image.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3957854406130268"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.fromiter(get_unique_N(nearest[0], 5), int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_nearest = np.empty(distance_matrix.shape[0])\n",
    "for line in nearest:\n",
    "    for entry in line:\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[2,5,1,6,3], [1,2,3,4,5]])\n",
    "np.argmin(a, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(3200)\n",
    "pic_a = features[train_matchings[idx,0]]\n",
    "pic_b = features[train_matchings[idx,1]]\n",
    "print(f\"same whales? {bool(train_matchings[idx,2])}\")    \n",
    "print(\"Index: \", idx)\n",
    "loss = model1.session.run(model1.distance, feed_dict={model1.X_1: np.array([pic_a]), model1.X_2: np.array([pic_b]), model1.Y: np.array([train_matchings[idx,2]])})\n",
    "print(loss)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(pic_a[0], cmap=\"gray\")\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(pic_b[0], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "for idx in tqdm(range(3200)):\n",
    "    loss = model1.session.run(model1.loss, feed_dict={model1.X_1: np.array([train_data[idx][0]]), model1.X_2: np.array([train_data[idx][1]]), model1.Y: np.array([train_labels[idx]])})\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_indices = [not (0.03<x<0.04) for x in losses]\n",
    "sum(outlier_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0,0,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(outlier_indices)):\n",
    "    if outlier_indices[i]:\n",
    "        print(f\"Dissimilarity: {losses[i]}\")\n",
    "        print(f\"Correct Label: {train_labels[i]}\")\n",
    "        print(\"Picture 1: \")\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.imshow(train_data[i,0,0], cmap=\"gray\")\n",
    "        plt.show()\n",
    "        print(\"Picture 2: \")\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.imshow(train_data[i,1,0], cmap=\"gray\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic_a= train_data[1441][0]\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(pic_a[0], cmap=\"gray\")\n",
    "plt.show()\n",
    "print(train_labels[1441])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.show_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"model 1\")\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(model1.hist['train_loss'][5::], label=\"Training\")\n",
    "plt.plot(model1.hist['val_loss'][5::], label=\"Validation\")\n",
    "\n",
    "plt.xlabel(\"Epoch\", fontsize=20)\n",
    "plt.ylabel(\"Loss\", fontsize=20)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\"\"\"\n",
    "print(\"model 2\")\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(model2.hist['train_loss'][5::], label=\"Training\")\n",
    "plt.plot(model2.hist['val_loss'][5::], label=\"Validation\")\n",
    "\n",
    "plt.xlabel(\"Epoch\", fontsize=20)\n",
    "plt.ylabel(\"Loss\", fontsize=20)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model1.hist[\"train_loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now plot the training and validation accuracies over the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"model 1\")\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.plot(model1.hist['train_accuracy'])\n",
    "plt.plot(model1.hist['val_accuracy'])\n",
    "\n",
    "plt.xlabel(\"Epoch\", fontsize=20)\n",
    "plt.ylabel(\"Accuracy\", fontsize=20)\n",
    "\n",
    "plt.show()\n",
    "\"\"\"\n",
    "print(\"model 2\")\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.plot(model2.hist['train_accuracy'])\n",
    "plt.plot(model2.hist['val_accuracy'])\n",
    "\n",
    "plt.xlabel(\"Epoch\", fontsize=20)\n",
    "plt.ylabel(\"Accuracy\", fontsize=20)\n",
    "\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_1 = model1.hist['train_accuracy'][-1]\n",
    "val_acc_1 = model1.hist['val_accuracy'][-1]\n",
    "\"\"\"\n",
    "train_acc_2 = model2.hist['train_accuracy'][-1]\n",
    "val_acc_2 = model2.hist['val_accuracy'][-1]\n",
    "\"\"\"\n",
    "print(f\"Training accuracy model 1: {train_acc_1:.3f}\")\n",
    "print(f\"Validation accuracy model 1: {val_acc_1:.3f}\")\n",
    "print()\n",
    "\"\"\"\n",
    "print(f\"Training accuracy model 2: {train_acc_2:.3f}\")\n",
    "print(f\"Validation accuracy model 2: {val_acc_2:.3f}\")\n",
    "print()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the losses and accuracies of the models in one plot to compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(model1.hist['train_loss'][5::], label=\"Training model 1\",\n",
    "        color=\"darkgreen\")\n",
    "plt.plot(model1.hist['val_loss'][5::], label=\"Validation model 1\",\n",
    "        color=\"darkgreen\", linestyle=\"--\")\n",
    "\"\"\"\n",
    "plt.plot(model2.hist['train_loss'][5::], label=\"Training model 2\",\n",
    "        color=\"royalblue\")\n",
    "plt.plot(model2.hist['val_loss'][5::], label=\"Validation model 2\",\n",
    "        color=\"royalblue\", linestyle=\"--\")\n",
    "\"\"\"\n",
    "plt.xlabel(\"Epoch\", fontsize=20)\n",
    "plt.ylabel(\"Loss\", fontsize=20)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the models\n",
    "Now, compare the final training and validation losses achieved by the different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.plot(model1.hist['train_accuracy'], label=\"Training model 1\",\n",
    "        color=\"darkgreen\")\n",
    "plt.plot(model1.hist['val_accuracy'], label=\"Validation model 1\",\n",
    "        color=\"darkgreen\", linestyle=\"--\")\n",
    "\n",
    "plt.plot(model2.hist['train_accuracy'], label=\"Training model 2\",\n",
    "        color=\"royalblue\")\n",
    "plt.plot(model2.hist['val_accuracy'], label=\"Validation model 2\",\n",
    "        color=\"royalblue\", linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"Epoch\", fontsize=20)\n",
    "plt.ylabel(\"Accuracy\", fontsize=20)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = NN_dropout_regularization.logits.eval({NN_dropout_regularization.X: test_data},\n",
    "                                        session=NN_dropout_regularization.session).argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices({\"feature\": train_data, \"label\": train_labels})\n",
    "val_data = tf.data.Dataset.from_tensor_slices({\"feature\": val_data, \"label\": val_labels})\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.output_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build batches\n",
    "batch_size = 500\n",
    "train_data.shuffle(30000)\n",
    "batches = dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "iterator = batches.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "no_of_batches = int(np.ceil(labels.shape[0] / batch_size))\n",
    "counter = 1\n",
    "for i in tqdm(range(no_of_batches)):\n",
    "    value = sess.run(next_element)\n",
    "    print(value[\"feature\"].shape)\n",
    "    print(counter)\n",
    "    counter+=1\n",
    "\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
